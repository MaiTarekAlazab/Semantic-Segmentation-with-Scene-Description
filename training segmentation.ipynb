{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "14c7edd3",
      "metadata": {
        "id": "14c7edd3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.utils import normalize\n",
        "import glob\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import ReLU,Concatenate,Input,AveragePooling2D, Activation, GlobalAveragePooling2D, Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Lambda,GlobalMaxPooling2D\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.metrics import MeanIoU \n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "lhqAQwjV11G9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhqAQwjV11G9",
        "outputId": "28c8e151-7593-401f-99f7-f403066d3ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "nll4nATG11JM",
      "metadata": {
        "id": "nll4nATG11JM"
      },
      "outputs": [],
      "source": [
        "input_dir = '/content/drive/Shared drives/ITI_Graduation_Project/dataset/data_semantics/training/image_2/'\n",
        "target_dir = '/content/drive/Shared drives/ITI_Graduation_Project/dataset/data_semantics/training/semantic/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ur03FM7b11LH",
      "metadata": {
        "id": "Ur03FM7b11LH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d27029",
      "metadata": {
        "id": "c7d27029"
      },
      "outputs": [],
      "source": [
        "#input_dir = 'dataset/data_semantics/training/image_2'\n",
        "#target_dir = 'dataset/data_semantics/training/semantic'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5bcb1ec8",
      "metadata": {
        "id": "5bcb1ec8"
      },
      "outputs": [],
      "source": [
        "train_images = []\n",
        "for img_path in sorted(glob.glob(os.path.join(input_dir, \"*.png\"))):\n",
        "    img = cv2.imread(img_path)       \n",
        "    img = cv2.resize(img, (128, 128))\n",
        "    cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    train_images.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "nqHGT5KcLw6G",
      "metadata": {
        "id": "nqHGT5KcLw6G"
      },
      "outputs": [],
      "source": [
        "train_masks = [] \n",
        "for mask_path in sorted(glob.glob(os.path.join(target_dir, \"*.png\"))):\n",
        "    mask = cv2.imread(mask_path,0)       \n",
        "    mask = cv2.resize(mask, (128, 128), interpolation = cv2.INTER_NEAREST)\n",
        "    cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "    train_masks.append(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d09113bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "d09113bf",
        "outputId": "dd4c8a76-b0f5-4900-af4e-aca30a6b0ab0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5e57b925d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(train_masks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5c35b3ff",
      "metadata": {
        "id": "5c35b3ff"
      },
      "outputs": [],
      "source": [
        "train_images = np.array(train_images , dtype='float32')\n",
        "train_masks = np.array(train_masks , dtype= 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b3bXyvrvqNo-",
      "metadata": {
        "id": "b3bXyvrvqNo-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_images, test_img, train_masks, test_masks = train_test_split(train_images, train_masks, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ee8a88c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8a88c9",
        "outputId": "0a527936-62e9-453a-c0eb-156a85861291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "n, h, w= train_masks.shape\n",
        "train_masks_reshaped = train_masks.reshape(-1,1)\n",
        "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
        "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
        "\n",
        "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
        "np.unique(train_masks_encoded_original_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ta6BLQupqgQo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta6BLQupqgQo",
        "outputId": "50547c5e-c227-4bfe-98f8-a09f9f9815f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "n, h, w= test_masks.shape\n",
        "test_masks_reshaped = test_masks.reshape(-1,1)\n",
        "test_masks_reshaped_encoded = labelencoder.fit_transform(test_masks_reshaped)\n",
        "test_masks_encoded_original_shape = test_masks_reshaped_encoded.reshape(n, h, w)\n",
        "test_masks = np.expand_dims(test_masks_encoded_original_shape, axis=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "srtuqqX1q2KB",
      "metadata": {
        "id": "srtuqqX1q2KB"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "test_img = test_img / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ef170e30",
      "metadata": {
        "id": "ef170e30"
      },
      "outputs": [],
      "source": [
        "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "Gj2xHYhPS9qP",
      "metadata": {
        "id": "Gj2xHYhPS9qP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "train_masks_cat = to_categorical(y1, num_classes=31)\n",
        "y_train_cat = train_masks_cat.reshape((y1.shape[0], y1.shape[1], y1.shape[2], 31))\n",
        "\n",
        "\n",
        "\n",
        "test_masks_cat = to_categorical(y_test, num_classes=31)\n",
        "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], 31))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4WQdZj1wrpkZ",
      "metadata": {
        "id": "4WQdZj1wrpkZ"
      },
      "outputs": [],
      "source": [
        "test_masks_cat = to_categorical(test_masks, num_classes=31)\n",
        "test_masks_cat = test_masks_cat.reshape((test_masks.shape[0], test_masks.shape[1], test_masks.shape[2], 31))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3f4cb3da",
      "metadata": {
        "id": "3f4cb3da"
      },
      "outputs": [],
      "source": [
        "#======================================================================================================================\n",
        "#losses\n",
        "def tversky(y_true, y_pred, smooth=1, alpha=0.7):\n",
        "    y_true_pos = K.flatten(y_true)\n",
        "    y_pred_pos = K.flatten(y_pred)\n",
        "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
        "    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n",
        "    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n",
        "    return (true_pos + smooth) / (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n",
        "\n",
        "\n",
        "def tversky_loss(y_true, y_pred):\n",
        "    return 1 - tversky(y_true, y_pred)\n",
        "\n",
        "\n",
        "def focal_tversky_loss(y_true, y_pred, gamma=0.75):\n",
        "    tv = tversky(y_true, y_pred)\n",
        "    return K.pow((1 - tv), gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4beeac3a",
      "metadata": {
        "id": "4beeac3a"
      },
      "outputs": [],
      "source": [
        "def jaccard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jaccard_coef_loss(y_true, y_pred):\n",
        "    return 1.0 - jaccard_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "667fddbd",
      "metadata": {
        "id": "667fddbd"
      },
      "outputs": [],
      "source": [
        "def dice_coef(y_true, y_pred):\n",
        "    #y_true = tf.cast(y_true, y_pred.dtype)\n",
        "    smooth = 1\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        " \n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f60c0ce8",
      "metadata": {
        "id": "f60c0ce8"
      },
      "outputs": [],
      "source": [
        "#======================================================================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3f1d7c5f",
      "metadata": {
        "id": "3f1d7c5f"
      },
      "outputs": [],
      "source": [
        "#Unet Model\n",
        "def convolution_operation(entered_input, filters=64):\n",
        "    # Taking first input and implementing the first conv block\n",
        "    conv1 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(entered_input)\n",
        "    batch_norm1 = BatchNormalization()(conv1)\n",
        "    act1 = ReLU()(batch_norm1)\n",
        "    \n",
        "    # Taking first input and implementing the second conv block\n",
        "    conv2 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(act1)\n",
        "    batch_norm2 = BatchNormalization()(conv2)\n",
        "    act2 = ReLU()(batch_norm2)\n",
        "    \n",
        "    return act2\n",
        "\n",
        "def encoder(entered_input, filters=64):\n",
        "    # Collect the start and end of each sub-block for normal pass and skip connections\n",
        "    enc1 = convolution_operation(entered_input, filters)\n",
        "    MaxPool1 = MaxPooling2D(strides = (2,2))(enc1)\n",
        "    return enc1, MaxPool1\n",
        "\n",
        "def decoder(entered_input, skip, filters=64):\n",
        "    # Upsampling and concatenating the essential features\n",
        "    Upsample = Conv2DTranspose(filters, (2, 2), strides=2, padding=\"same\")(entered_input)\n",
        "    Connect_Skip = Concatenate()([Upsample, skip])\n",
        "    out = convolution_operation(Connect_Skip, filters)\n",
        "    return out\n",
        "\n",
        "def U_Net(Image_Size,num_classes):\n",
        "    # Take the image size and shape\n",
        "    input1 = Input(Image_Size)\n",
        "    \n",
        "    # Construct the encoder blocks\n",
        "    skip1, encoder_1 = encoder(input1, 64)\n",
        "    skip2, encoder_2 = encoder(encoder_1, 64*2)\n",
        "    skip3, encoder_3 = encoder(encoder_2, 64*4)\n",
        "    skip4, encoder_4 = encoder(encoder_3, 64*8)\n",
        "    \n",
        "    # Preparing the next block\n",
        "    conv_block = convolution_operation(encoder_4, 64*16)\n",
        "    \n",
        "    # Construct the decoder blocks\n",
        "    decoder_1 = decoder(conv_block, skip4, 64*8)\n",
        "    decoder_2 = decoder(decoder_1, skip3, 64*4)\n",
        "    decoder_3 = decoder(decoder_2, skip2, 64*2)\n",
        "    decoder_4 = decoder(decoder_3, skip1, 64)\n",
        "    \n",
        "    out = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(decoder_4)\n",
        "\n",
        "    model = Model(inputs=[input1], outputs=[out])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5a5096ce",
      "metadata": {
        "id": "5a5096ce"
      },
      "outputs": [],
      "source": [
        "input_shape = (128, 128, 3)\n",
        "num_classes=31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2452ddda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2452ddda",
        "outputId": "921715e8-444b-46cf-bb94-a088cabec806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 128, 128, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 128, 64  256        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, 128, 128, 64  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 128, 128, 64  36928       ['re_lu[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128, 128, 64  256        ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)                 (None, 128, 128, 64  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 64, 64, 64)   0           ['re_lu_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)                 (None, 64, 64, 128)  0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 64, 64, 128)  147584      ['re_lu_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 64, 64, 128)  512        ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 64, 64, 128)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0          ['re_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)                 (None, 32, 32, 256)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 256)  590080      ['re_lu_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 256)  1024       ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_5 (ReLU)                 (None, 32, 32, 256)  0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0          ['re_lu_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_6 (ReLU)                 (None, 16, 16, 512)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 16, 16, 512)  2359808     ['re_lu_6[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 16, 512)  2048       ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_7 (ReLU)                 (None, 16, 16, 512)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)   0           ['re_lu_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 8, 8, 1024)   4719616     ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 8, 8, 1024)  4096        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_8 (ReLU)                 (None, 8, 8, 1024)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 8, 8, 1024)   9438208     ['re_lu_8[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 8, 8, 1024)  4096        ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_9 (ReLU)                 (None, 8, 8, 1024)   0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 16, 16, 512)  2097664    ['re_lu_9[0][0]']                \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 16, 16, 1024  0           ['conv2d_transpose[0][0]',       \n",
            "                                )                                 're_lu_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 16, 16, 512)  4719104     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_10 (ReLU)                (None, 16, 16, 512)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 16, 16, 512)  2359808     ['re_lu_10[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_11 (ReLU)                (None, 16, 16, 512)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 32, 32, 256)  524544     ['re_lu_11[0][0]']               \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 32, 512)  0           ['conv2d_transpose_1[0][0]',     \n",
            "                                                                  're_lu_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 256)  1179904     ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_12 (ReLU)                (None, 32, 32, 256)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 256)  590080      ['re_lu_12[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_13 (ReLU)                (None, 32, 32, 256)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 128)  131200     ['re_lu_13[0][0]']               \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 64, 64, 256)  0           ['conv2d_transpose_2[0][0]',     \n",
            "                                                                  're_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 64, 64, 128)  295040      ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_14 (ReLU)                (None, 64, 64, 128)  0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['re_lu_14[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " re_lu_15 (ReLU)                (None, 64, 64, 128)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 128, 128, 64  32832      ['re_lu_15[0][0]']               \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 128, 128, 12  0           ['conv2d_transpose_3[0][0]',     \n",
            "                                8)                                're_lu_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 128, 128, 64  73792       ['concatenate_3[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 128, 128, 64  256        ['conv2d_16[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " re_lu_16 (ReLU)                (None, 128, 128, 64  0           ['batch_normalization_16[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 128, 128, 64  36928       ['re_lu_16[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 128, 128, 64  256        ['conv2d_17[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " re_lu_17 (ReLU)                (None, 128, 128, 64  0           ['batch_normalization_17[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 128, 128, 31  2015        ['re_lu_17[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,057,247\n",
            "Trainable params: 31,045,471\n",
            "Non-trainable params: 11,776\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_UNet = U_Net(input_shape,num_classes)\n",
        "model_UNet.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4aed9b4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aed9b4b",
        "outputId": "2525663d-954e-4790-8fae-3fdb952c6cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "18/18 [==============================] - 20s 241ms/step - loss: 0.7953 - acc: 0.5001 - dice_coef: 0.3280 - jaccard_coef: 0.2047 - val_loss: 0.8364 - val_acc: 0.3581 - val_dice_coef: 0.2811 - val_jaccard_coef: 0.1636 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 4s 215ms/step - loss: 0.6245 - acc: 0.6442 - dice_coef: 0.5437 - jaccard_coef: 0.3755 - val_loss: 0.7804 - val_acc: 0.3940 - val_dice_coef: 0.3601 - val_jaccard_coef: 0.2196 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 4s 221ms/step - loss: 0.5469 - acc: 0.6683 - dice_coef: 0.6218 - jaccard_coef: 0.4531 - val_loss: 0.7505 - val_acc: 0.4341 - val_dice_coef: 0.3993 - val_jaccard_coef: 0.2495 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 3s 152ms/step - loss: 0.5100 - acc: 0.6767 - dice_coef: 0.6557 - jaccard_coef: 0.4900 - val_loss: 0.7936 - val_acc: 0.4114 - val_dice_coef: 0.3420 - val_jaccard_coef: 0.2064 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.5000 - acc: 0.6769 - dice_coef: 0.6645 - jaccard_coef: 0.5000 - val_loss: 0.8051 - val_acc: 0.3801 - val_dice_coef: 0.3262 - val_jaccard_coef: 0.1949 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 3s 150ms/step - loss: 0.4886 - acc: 0.6826 - dice_coef: 0.6751 - jaccard_coef: 0.5114 - val_loss: 0.8421 - val_acc: 0.2901 - val_dice_coef: 0.2728 - val_jaccard_coef: 0.1579 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 3s 151ms/step - loss: 0.4824 - acc: 0.6857 - dice_coef: 0.6789 - jaccard_coef: 0.5176 - val_loss: 0.8386 - val_acc: 0.2910 - val_dice_coef: 0.2779 - val_jaccard_coef: 0.1614 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 3s 152ms/step - loss: 0.4629 - acc: 0.7081 - dice_coef: 0.6973 - jaccard_coef: 0.5371 - val_loss: 0.8260 - val_acc: 0.3190 - val_dice_coef: 0.2963 - val_jaccard_coef: 0.1740 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 3s 151ms/step - loss: 0.4465 - acc: 0.7176 - dice_coef: 0.7104 - jaccard_coef: 0.5535 - val_loss: 0.8076 - val_acc: 0.3266 - val_dice_coef: 0.3225 - val_jaccard_coef: 0.1924 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 4s 219ms/step - loss: 0.4548 - acc: 0.7093 - dice_coef: 0.7041 - jaccard_coef: 0.5452 - val_loss: 0.7050 - val_acc: 0.5900 - val_dice_coef: 0.4556 - val_jaccard_coef: 0.2950 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 4s 210ms/step - loss: 0.4398 - acc: 0.7206 - dice_coef: 0.7167 - jaccard_coef: 0.5602 - val_loss: 0.6762 - val_acc: 0.5506 - val_dice_coef: 0.4891 - val_jaccard_coef: 0.3238 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 4s 209ms/step - loss: 0.4402 - acc: 0.7203 - dice_coef: 0.7165 - jaccard_coef: 0.5598 - val_loss: 0.6684 - val_acc: 0.6146 - val_dice_coef: 0.4979 - val_jaccard_coef: 0.3316 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 3s 151ms/step - loss: 0.4341 - acc: 0.7233 - dice_coef: 0.7204 - jaccard_coef: 0.5659 - val_loss: 0.7499 - val_acc: 0.4261 - val_dice_coef: 0.4001 - val_jaccard_coef: 0.2501 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 3s 152ms/step - loss: 0.4403 - acc: 0.7198 - dice_coef: 0.7165 - jaccard_coef: 0.5597 - val_loss: 0.6958 - val_acc: 0.5481 - val_dice_coef: 0.4665 - val_jaccard_coef: 0.3042 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 4s 216ms/step - loss: 0.4237 - acc: 0.7350 - dice_coef: 0.7294 - jaccard_coef: 0.5763 - val_loss: 0.5973 - val_acc: 0.5906 - val_dice_coef: 0.5738 - val_jaccard_coef: 0.4027 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 3s 152ms/step - loss: 0.4141 - acc: 0.7431 - dice_coef: 0.7376 - jaccard_coef: 0.5859 - val_loss: 0.7882 - val_acc: 0.3489 - val_dice_coef: 0.3495 - val_jaccard_coef: 0.2118 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.3835 - acc: 0.7656 - dice_coef: 0.7612 - jaccard_coef: 0.6165 - val_loss: 0.7928 - val_acc: 0.3437 - val_dice_coef: 0.3432 - val_jaccard_coef: 0.2072 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 3s 152ms/step - loss: 0.3841 - acc: 0.7645 - dice_coef: 0.7609 - jaccard_coef: 0.6159 - val_loss: 0.6536 - val_acc: 0.5331 - val_dice_coef: 0.5135 - val_jaccard_coef: 0.3464 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.3709 - acc: 0.7742 - dice_coef: 0.7713 - jaccard_coef: 0.6291 - val_loss: 0.7948 - val_acc: 0.3405 - val_dice_coef: 0.3405 - val_jaccard_coef: 0.2052 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.3688 - acc: 0.7758 - dice_coef: 0.7734 - jaccard_coef: 0.6312 - val_loss: 0.7920 - val_acc: 0.3446 - val_dice_coef: 0.3443 - val_jaccard_coef: 0.2080 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.3721 - acc: 0.7731 - dice_coef: 0.7705 - jaccard_coef: 0.6279 - val_loss: 0.6433 - val_acc: 0.5478 - val_dice_coef: 0.5254 - val_jaccard_coef: 0.3567 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.3667 - acc: 0.7769 - dice_coef: 0.7747 - jaccard_coef: 0.6333 - val_loss: 0.7442 - val_acc: 0.4094 - val_dice_coef: 0.4071 - val_jaccard_coef: 0.2558 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 3s 153ms/step - loss: 0.3616 - acc: 0.7802 - dice_coef: 0.7781 - jaccard_coef: 0.6384 - val_loss: 0.6988 - val_acc: 0.4659 - val_dice_coef: 0.4624 - val_jaccard_coef: 0.3012 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 4s 222ms/step - loss: 0.3606 - acc: 0.7809 - dice_coef: 0.7791 - jaccard_coef: 0.6394 - val_loss: 0.5508 - val_acc: 0.6155 - val_dice_coef: 0.6197 - val_jaccard_coef: 0.4492 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 3s 154ms/step - loss: 0.3575 - acc: 0.7836 - dice_coef: 0.7813 - jaccard_coef: 0.6425 - val_loss: 0.5961 - val_acc: 0.5842 - val_dice_coef: 0.5754 - val_jaccard_coef: 0.4039 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 3s 154ms/step - loss: 0.3574 - acc: 0.7831 - dice_coef: 0.7811 - jaccard_coef: 0.6426 - val_loss: 0.7086 - val_acc: 0.4510 - val_dice_coef: 0.4510 - val_jaccard_coef: 0.2914 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 3s 154ms/step - loss: 0.3515 - acc: 0.7880 - dice_coef: 0.7861 - jaccard_coef: 0.6485 - val_loss: 0.5963 - val_acc: 0.5827 - val_dice_coef: 0.5751 - val_jaccard_coef: 0.4037 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 4s 220ms/step - loss: 0.3618 - acc: 0.7798 - dice_coef: 0.7778 - jaccard_coef: 0.6382 - val_loss: 0.4386 - val_acc: 0.7219 - val_dice_coef: 0.7189 - val_jaccard_coef: 0.5614 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 3s 154ms/step - loss: 0.3563 - acc: 0.7837 - dice_coef: 0.7820 - jaccard_coef: 0.6437 - val_loss: 0.5694 - val_acc: 0.5970 - val_dice_coef: 0.6020 - val_jaccard_coef: 0.4306 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 3s 154ms/step - loss: 0.3550 - acc: 0.7843 - dice_coef: 0.7827 - jaccard_coef: 0.6450 - val_loss: 0.5670 - val_acc: 0.6054 - val_dice_coef: 0.6018 - val_jaccard_coef: 0.4330 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 3s 155ms/step - loss: 0.3501 - acc: 0.7890 - dice_coef: 0.7873 - jaccard_coef: 0.6499 - val_loss: 0.5011 - val_acc: 0.6668 - val_dice_coef: 0.6652 - val_jaccard_coef: 0.4989 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 4s 215ms/step - loss: 0.3517 - acc: 0.7868 - dice_coef: 0.7853 - jaccard_coef: 0.6483 - val_loss: 0.3825 - val_acc: 0.7647 - val_dice_coef: 0.7633 - val_jaccard_coef: 0.6175 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 4s 211ms/step - loss: 0.3372 - acc: 0.7981 - dice_coef: 0.7967 - jaccard_coef: 0.6628 - val_loss: 0.3748 - val_acc: 0.7730 - val_dice_coef: 0.7691 - val_jaccard_coef: 0.6252 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 4s 215ms/step - loss: 0.3324 - acc: 0.8015 - dice_coef: 0.8001 - jaccard_coef: 0.6676 - val_loss: 0.3653 - val_acc: 0.7781 - val_dice_coef: 0.7763 - val_jaccard_coef: 0.6347 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 4s 217ms/step - loss: 0.3335 - acc: 0.8009 - dice_coef: 0.7992 - jaccard_coef: 0.6665 - val_loss: 0.3561 - val_acc: 0.7871 - val_dice_coef: 0.7833 - val_jaccard_coef: 0.6439 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3374 - acc: 0.7970 - dice_coef: 0.7955 - jaccard_coef: 0.6626 - val_loss: 0.4757 - val_acc: 0.6928 - val_dice_coef: 0.6878 - val_jaccard_coef: 0.5243 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 3s 155ms/step - loss: 0.3301 - acc: 0.8032 - dice_coef: 0.8017 - jaccard_coef: 0.6699 - val_loss: 0.5178 - val_acc: 0.6525 - val_dice_coef: 0.6502 - val_jaccard_coef: 0.4822 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 3s 155ms/step - loss: 0.3235 - acc: 0.8077 - dice_coef: 0.8063 - jaccard_coef: 0.6765 - val_loss: 0.6099 - val_acc: 0.5619 - val_dice_coef: 0.5611 - val_jaccard_coef: 0.3901 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 4s 225ms/step - loss: 0.3289 - acc: 0.8037 - dice_coef: 0.8023 - jaccard_coef: 0.6711 - val_loss: 0.3446 - val_acc: 0.7934 - val_dice_coef: 0.7916 - val_jaccard_coef: 0.6554 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 3s 155ms/step - loss: 0.3423 - acc: 0.7941 - dice_coef: 0.7927 - jaccard_coef: 0.6577 - val_loss: 0.3465 - val_acc: 0.7952 - val_dice_coef: 0.7904 - val_jaccard_coef: 0.6535 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3384 - acc: 0.7966 - dice_coef: 0.7952 - jaccard_coef: 0.6616 - val_loss: 0.3564 - val_acc: 0.7839 - val_dice_coef: 0.7831 - val_jaccard_coef: 0.6436 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 4s 214ms/step - loss: 0.3292 - acc: 0.8036 - dice_coef: 0.8024 - jaccard_coef: 0.6708 - val_loss: 0.3360 - val_acc: 0.7997 - val_dice_coef: 0.7980 - val_jaccard_coef: 0.6640 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 3s 156ms/step - loss: 0.3295 - acc: 0.8031 - dice_coef: 0.8017 - jaccard_coef: 0.6705 - val_loss: 0.3750 - val_acc: 0.7714 - val_dice_coef: 0.7690 - val_jaccard_coef: 0.6250 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3243 - acc: 0.8064 - dice_coef: 0.8052 - jaccard_coef: 0.6757 - val_loss: 0.4019 - val_acc: 0.7502 - val_dice_coef: 0.7484 - val_jaccard_coef: 0.5981 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3295 - acc: 0.8034 - dice_coef: 0.8022 - jaccard_coef: 0.6705 - val_loss: 0.3548 - val_acc: 0.7854 - val_dice_coef: 0.7841 - val_jaccard_coef: 0.6452 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.3185 - acc: 0.8103 - dice_coef: 0.8092 - jaccard_coef: 0.6815 - val_loss: 0.3801 - val_acc: 0.7674 - val_dice_coef: 0.7651 - val_jaccard_coef: 0.6199 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.3225 - acc: 0.8083 - dice_coef: 0.8070 - jaccard_coef: 0.6775 - val_loss: 0.3847 - val_acc: 0.7653 - val_dice_coef: 0.7613 - val_jaccard_coef: 0.6153 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 3s 156ms/step - loss: 0.3200 - acc: 0.8102 - dice_coef: 0.8088 - jaccard_coef: 0.6800 - val_loss: 0.3804 - val_acc: 0.7666 - val_dice_coef: 0.7646 - val_jaccard_coef: 0.6196 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3186 - acc: 0.8110 - dice_coef: 0.8099 - jaccard_coef: 0.6814 - val_loss: 0.3566 - val_acc: 0.7864 - val_dice_coef: 0.7829 - val_jaccard_coef: 0.6434 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 3s 156ms/step - loss: 0.3115 - acc: 0.8157 - dice_coef: 0.8146 - jaccard_coef: 0.6885 - val_loss: 0.3436 - val_acc: 0.7951 - val_dice_coef: 0.7924 - val_jaccard_coef: 0.6564 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3133 - acc: 0.8141 - dice_coef: 0.8129 - jaccard_coef: 0.6867 - val_loss: 0.4091 - val_acc: 0.7459 - val_dice_coef: 0.7423 - val_jaccard_coef: 0.5909 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 4s 222ms/step - loss: 0.3183 - acc: 0.8114 - dice_coef: 0.8103 - jaccard_coef: 0.6817 - val_loss: 0.3340 - val_acc: 0.8008 - val_dice_coef: 0.7994 - val_jaccard_coef: 0.6660 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 4s 219ms/step - loss: 0.3109 - acc: 0.8162 - dice_coef: 0.8151 - jaccard_coef: 0.6891 - val_loss: 0.3161 - val_acc: 0.8136 - val_dice_coef: 0.8122 - val_jaccard_coef: 0.6839 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3086 - acc: 0.8172 - dice_coef: 0.8162 - jaccard_coef: 0.6914 - val_loss: 0.3299 - val_acc: 0.8034 - val_dice_coef: 0.8022 - val_jaccard_coef: 0.6701 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3173 - acc: 0.8120 - dice_coef: 0.8109 - jaccard_coef: 0.6827 - val_loss: 0.4171 - val_acc: 0.7354 - val_dice_coef: 0.7365 - val_jaccard_coef: 0.5829 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3135 - acc: 0.8142 - dice_coef: 0.8131 - jaccard_coef: 0.6865 - val_loss: 0.3239 - val_acc: 0.8075 - val_dice_coef: 0.8066 - val_jaccard_coef: 0.6761 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.3055 - acc: 0.8199 - dice_coef: 0.8189 - jaccard_coef: 0.6945 - val_loss: 0.4226 - val_acc: 0.7362 - val_dice_coef: 0.7316 - val_jaccard_coef: 0.5774 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.3092 - acc: 0.8171 - dice_coef: 0.8161 - jaccard_coef: 0.6908 - val_loss: 0.3623 - val_acc: 0.7833 - val_dice_coef: 0.7787 - val_jaccard_coef: 0.6377 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3048 - acc: 0.8203 - dice_coef: 0.8193 - jaccard_coef: 0.6952 - val_loss: 0.3435 - val_acc: 0.7938 - val_dice_coef: 0.7923 - val_jaccard_coef: 0.6565 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3148 - acc: 0.8136 - dice_coef: 0.8125 - jaccard_coef: 0.6852 - val_loss: 0.3727 - val_acc: 0.7771 - val_dice_coef: 0.7710 - val_jaccard_coef: 0.6273 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.3091 - acc: 0.8173 - dice_coef: 0.8163 - jaccard_coef: 0.6909 - val_loss: 0.4350 - val_acc: 0.7234 - val_dice_coef: 0.7220 - val_jaccard_coef: 0.5650 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3101 - acc: 0.8168 - dice_coef: 0.8159 - jaccard_coef: 0.6899 - val_loss: 0.3267 - val_acc: 0.8057 - val_dice_coef: 0.8047 - val_jaccard_coef: 0.6733 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.3027 - acc: 0.8217 - dice_coef: 0.8208 - jaccard_coef: 0.6973 - val_loss: 0.3954 - val_acc: 0.7596 - val_dice_coef: 0.7535 - val_jaccard_coef: 0.6046 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3122 - acc: 0.8151 - dice_coef: 0.8141 - jaccard_coef: 0.6878 - val_loss: 0.3407 - val_acc: 0.7953 - val_dice_coef: 0.7944 - val_jaccard_coef: 0.6593 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.3018 - acc: 0.8225 - dice_coef: 0.8216 - jaccard_coef: 0.6982 - val_loss: 0.3338 - val_acc: 0.8014 - val_dice_coef: 0.7994 - val_jaccard_coef: 0.6662 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3023 - acc: 0.8222 - dice_coef: 0.8212 - jaccard_coef: 0.6977 - val_loss: 0.3412 - val_acc: 0.7950 - val_dice_coef: 0.7942 - val_jaccard_coef: 0.6588 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.3068 - acc: 0.8187 - dice_coef: 0.8179 - jaccard_coef: 0.6932 - val_loss: 0.4163 - val_acc: 0.7371 - val_dice_coef: 0.7371 - val_jaccard_coef: 0.5837 - lr: 0.0010\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3009 - acc: 0.8232 - dice_coef: 0.8224 - jaccard_coef: 0.6991 - val_loss: 0.3246 - val_acc: 0.8069 - val_dice_coef: 0.8062 - val_jaccard_coef: 0.6754 - lr: 0.0010\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.3016 - acc: 0.8224 - dice_coef: 0.8217 - jaccard_coef: 0.6984 - val_loss: 0.3162 - val_acc: 0.8131 - val_dice_coef: 0.8121 - val_jaccard_coef: 0.6838 - lr: 0.0010\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 3s 157ms/step - loss: 0.2991 - acc: 0.8237 - dice_coef: 0.8229 - jaccard_coef: 0.7009 - val_loss: 0.3204 - val_acc: 0.8097 - val_dice_coef: 0.8091 - val_jaccard_coef: 0.6796 - lr: 0.0010\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2955 - acc: 0.8263 - dice_coef: 0.8255 - jaccard_coef: 0.7045 - val_loss: 0.3404 - val_acc: 0.7960 - val_dice_coef: 0.7947 - val_jaccard_coef: 0.6596 - lr: 0.0010\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.2937 - acc: 0.8281 - dice_coef: 0.8273 - jaccard_coef: 0.7063 - val_loss: 0.3183 - val_acc: 0.8117 - val_dice_coef: 0.8106 - val_jaccard_coef: 0.6817 - lr: 0.0010\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 4s 221ms/step - loss: 0.2956 - acc: 0.8268 - dice_coef: 0.8261 - jaccard_coef: 0.7044 - val_loss: 0.3151 - val_acc: 0.8143 - val_dice_coef: 0.8129 - val_jaccard_coef: 0.6849 - lr: 0.0010\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.3032 - acc: 0.8214 - dice_coef: 0.8207 - jaccard_coef: 0.6968 - val_loss: 0.3521 - val_acc: 0.7898 - val_dice_coef: 0.7861 - val_jaccard_coef: 0.6479 - lr: 0.0010\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.2972 - acc: 0.8258 - dice_coef: 0.8250 - jaccard_coef: 0.7028 - val_loss: 0.3197 - val_acc: 0.8102 - val_dice_coef: 0.8095 - val_jaccard_coef: 0.6803 - lr: 0.0010\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.3034 - acc: 0.8210 - dice_coef: 0.8202 - jaccard_coef: 0.6966 - val_loss: 0.3250 - val_acc: 0.8080 - val_dice_coef: 0.8059 - val_jaccard_coef: 0.6750 - lr: 0.0010\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2984 - acc: 0.8248 - dice_coef: 0.8239 - jaccard_coef: 0.7016 - val_loss: 0.3263 - val_acc: 0.8057 - val_dice_coef: 0.8048 - val_jaccard_coef: 0.6737 - lr: 0.0010\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 4s 221ms/step - loss: 0.2944 - acc: 0.8277 - dice_coef: 0.8269 - jaccard_coef: 0.7056 - val_loss: 0.3120 - val_acc: 0.8162 - val_dice_coef: 0.8151 - val_jaccard_coef: 0.6880 - lr: 0.0010\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2893 - acc: 0.8310 - dice_coef: 0.8302 - jaccard_coef: 0.7107 - val_loss: 0.3374 - val_acc: 0.7989 - val_dice_coef: 0.7968 - val_jaccard_coef: 0.6626 - lr: 0.0010\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2893 - acc: 0.8308 - dice_coef: 0.8301 - jaccard_coef: 0.7107 - val_loss: 0.3582 - val_acc: 0.7825 - val_dice_coef: 0.7816 - val_jaccard_coef: 0.6418 - lr: 0.0010\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 4s 218ms/step - loss: 0.2906 - acc: 0.8303 - dice_coef: 0.8296 - jaccard_coef: 0.7094 - val_loss: 0.3116 - val_acc: 0.8161 - val_dice_coef: 0.8154 - val_jaccard_coef: 0.6884 - lr: 0.0010\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 3s 158ms/step - loss: 0.2886 - acc: 0.8313 - dice_coef: 0.8307 - jaccard_coef: 0.7114 - val_loss: 0.3126 - val_acc: 0.8158 - val_dice_coef: 0.8147 - val_jaccard_coef: 0.6874 - lr: 0.0010\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2853 - acc: 0.8334 - dice_coef: 0.8328 - jaccard_coef: 0.7147 - val_loss: 0.3139 - val_acc: 0.8146 - val_dice_coef: 0.8138 - val_jaccard_coef: 0.6861 - lr: 0.0010\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2859 - acc: 0.8332 - dice_coef: 0.8325 - jaccard_coef: 0.7141 - val_loss: 0.3235 - val_acc: 0.8075 - val_dice_coef: 0.8068 - val_jaccard_coef: 0.6765 - lr: 0.0010\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2951 - acc: 0.8270 - dice_coef: 0.8262 - jaccard_coef: 0.7049 - val_loss: 0.3819 - val_acc: 0.7665 - val_dice_coef: 0.7636 - val_jaccard_coef: 0.6181 - lr: 0.0010\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2889 - acc: 0.8312 - dice_coef: 0.8306 - jaccard_coef: 0.7111 - val_loss: 0.3239 - val_acc: 0.8075 - val_dice_coef: 0.8065 - val_jaccard_coef: 0.6761 - lr: 0.0010\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 4s 235ms/step - loss: 0.2896 - acc: 0.8307 - dice_coef: 0.8301 - jaccard_coef: 0.7104 - val_loss: 0.3099 - val_acc: 0.8172 - val_dice_coef: 0.8166 - val_jaccard_coef: 0.6901 - lr: 0.0010\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2854 - acc: 0.8332 - dice_coef: 0.8326 - jaccard_coef: 0.7146 - val_loss: 0.3145 - val_acc: 0.8140 - val_dice_coef: 0.8133 - val_jaccard_coef: 0.6855 - lr: 0.0010\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2840 - acc: 0.8348 - dice_coef: 0.8341 - jaccard_coef: 0.7160 - val_loss: 0.3293 - val_acc: 0.8034 - val_dice_coef: 0.8027 - val_jaccard_coef: 0.6707 - lr: 0.0010\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2818 - acc: 0.8357 - dice_coef: 0.8350 - jaccard_coef: 0.7182 - val_loss: 0.3175 - val_acc: 0.8126 - val_dice_coef: 0.8113 - val_jaccard_coef: 0.6825 - lr: 0.0010\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2858 - acc: 0.8334 - dice_coef: 0.8328 - jaccard_coef: 0.7142 - val_loss: 0.3202 - val_acc: 0.8101 - val_dice_coef: 0.8093 - val_jaccard_coef: 0.6798 - lr: 0.0010\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2890 - acc: 0.8311 - dice_coef: 0.8304 - jaccard_coef: 0.7110 - val_loss: 0.3506 - val_acc: 0.7901 - val_dice_coef: 0.7872 - val_jaccard_coef: 0.6494 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 4s 222ms/step - loss: 0.2836 - acc: 0.8349 - dice_coef: 0.8341 - jaccard_coef: 0.7164 - val_loss: 0.3029 - val_acc: 0.8222 - val_dice_coef: 0.8215 - val_jaccard_coef: 0.6971 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2856 - acc: 0.8336 - dice_coef: 0.8330 - jaccard_coef: 0.7144 - val_loss: 0.3120 - val_acc: 0.8155 - val_dice_coef: 0.8150 - val_jaccard_coef: 0.6880 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2817 - acc: 0.8361 - dice_coef: 0.8356 - jaccard_coef: 0.7183 - val_loss: 0.3189 - val_acc: 0.8113 - val_dice_coef: 0.8102 - val_jaccard_coef: 0.6811 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2804 - acc: 0.8372 - dice_coef: 0.8367 - jaccard_coef: 0.7196 - val_loss: 0.3155 - val_acc: 0.8143 - val_dice_coef: 0.8126 - val_jaccard_coef: 0.6845 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2803 - acc: 0.8368 - dice_coef: 0.8363 - jaccard_coef: 0.7197 - val_loss: 0.3031 - val_acc: 0.8221 - val_dice_coef: 0.8213 - val_jaccard_coef: 0.6969 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2802 - acc: 0.8371 - dice_coef: 0.8365 - jaccard_coef: 0.7198 - val_loss: 0.3130 - val_acc: 0.8150 - val_dice_coef: 0.8144 - val_jaccard_coef: 0.6870 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2777 - acc: 0.8384 - dice_coef: 0.8378 - jaccard_coef: 0.7223 - val_loss: 0.3106 - val_acc: 0.8171 - val_dice_coef: 0.8161 - val_jaccard_coef: 0.6894 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2751 - acc: 0.8406 - dice_coef: 0.8400 - jaccard_coef: 0.7249 - val_loss: 0.3042 - val_acc: 0.8210 - val_dice_coef: 0.8205 - val_jaccard_coef: 0.6958 - lr: 0.0010\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2766 - acc: 0.8396 - dice_coef: 0.8390 - jaccard_coef: 0.7234 - val_loss: 0.3345 - val_acc: 0.8018 - val_dice_coef: 0.7990 - val_jaccard_coef: 0.6655 - lr: 0.0010\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2754 - acc: 0.8402 - dice_coef: 0.8396 - jaccard_coef: 0.7246 - val_loss: 0.3063 - val_acc: 0.8197 - val_dice_coef: 0.8191 - val_jaccard_coef: 0.6937 - lr: 0.0010\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2751 - acc: 0.8405 - dice_coef: 0.8398 - jaccard_coef: 0.7249 - val_loss: 0.3140 - val_acc: 0.8144 - val_dice_coef: 0.8137 - val_jaccard_coef: 0.6860 - lr: 0.0010\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2752 - acc: 0.8404 - dice_coef: 0.8398 - jaccard_coef: 0.7248 - val_loss: 0.3164 - val_acc: 0.8135 - val_dice_coef: 0.8120 - val_jaccard_coef: 0.6836 - lr: 0.0010\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2741 - acc: 0.8411 - dice_coef: 0.8405 - jaccard_coef: 0.7259 - val_loss: 0.3096 - val_acc: 0.8175 - val_dice_coef: 0.8167 - val_jaccard_coef: 0.6904 - lr: 0.0010\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 4s 227ms/step - loss: 0.2744 - acc: 0.8410 - dice_coef: 0.8404 - jaccard_coef: 0.7256 - val_loss: 0.3009 - val_acc: 0.8236 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6991 - lr: 0.0010\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2727 - acc: 0.8420 - dice_coef: 0.8414 - jaccard_coef: 0.7273 - val_loss: 0.3098 - val_acc: 0.8178 - val_dice_coef: 0.8167 - val_jaccard_coef: 0.6902 - lr: 0.0010\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2744 - acc: 0.8412 - dice_coef: 0.8405 - jaccard_coef: 0.7256 - val_loss: 0.3345 - val_acc: 0.7996 - val_dice_coef: 0.7989 - val_jaccard_coef: 0.6655 - lr: 0.0010\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2761 - acc: 0.8397 - dice_coef: 0.8390 - jaccard_coef: 0.7239 - val_loss: 0.3440 - val_acc: 0.7928 - val_dice_coef: 0.7921 - val_jaccard_coef: 0.6560 - lr: 0.0010\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2751 - acc: 0.8402 - dice_coef: 0.8396 - jaccard_coef: 0.7249 - val_loss: 0.3069 - val_acc: 0.8193 - val_dice_coef: 0.8187 - val_jaccard_coef: 0.6931 - lr: 0.0010\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2736 - acc: 0.8414 - dice_coef: 0.8409 - jaccard_coef: 0.7264 - val_loss: 0.3130 - val_acc: 0.8151 - val_dice_coef: 0.8144 - val_jaccard_coef: 0.6870 - lr: 0.0010\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2731 - acc: 0.8414 - dice_coef: 0.8410 - jaccard_coef: 0.7269 - val_loss: 0.3079 - val_acc: 0.8189 - val_dice_coef: 0.8180 - val_jaccard_coef: 0.6921 - lr: 0.0010\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 3s 163ms/step - loss: 0.2737 - acc: 0.8413 - dice_coef: 0.8408 - jaccard_coef: 0.7263 - val_loss: 0.3039 - val_acc: 0.8216 - val_dice_coef: 0.8208 - val_jaccard_coef: 0.6961 - lr: 0.0010\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2740 - acc: 0.8413 - dice_coef: 0.8407 - jaccard_coef: 0.7260 - val_loss: 0.3252 - val_acc: 0.8062 - val_dice_coef: 0.8057 - val_jaccard_coef: 0.6748 - lr: 0.0010\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2823 - acc: 0.8353 - dice_coef: 0.8348 - jaccard_coef: 0.7177 - val_loss: 0.3182 - val_acc: 0.8113 - val_dice_coef: 0.8107 - val_jaccard_coef: 0.6818 - lr: 0.0010\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2868 - acc: 0.8325 - dice_coef: 0.8319 - jaccard_coef: 0.7132 - val_loss: 0.3239 - val_acc: 0.8076 - val_dice_coef: 0.8067 - val_jaccard_coef: 0.6761 - lr: 0.0010\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2799 - acc: 0.8372 - dice_coef: 0.8367 - jaccard_coef: 0.7201 - val_loss: 0.3115 - val_acc: 0.8160 - val_dice_coef: 0.8154 - val_jaccard_coef: 0.6885 - lr: 0.0010\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2789 - acc: 0.8376 - dice_coef: 0.8372 - jaccard_coef: 0.7211 - val_loss: 0.3109 - val_acc: 0.8162 - val_dice_coef: 0.8158 - val_jaccard_coef: 0.6891 - lr: 0.0010\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2812 - acc: 0.8364 - dice_coef: 0.8358 - jaccard_coef: 0.7188 - val_loss: 0.3231 - val_acc: 0.8082 - val_dice_coef: 0.8072 - val_jaccard_coef: 0.6769 - lr: 0.0010\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2827 - acc: 0.8353 - dice_coef: 0.8347 - jaccard_coef: 0.7173 - val_loss: 0.3283 - val_acc: 0.8049 - val_dice_coef: 0.8036 - val_jaccard_coef: 0.6717 - lr: 0.0010\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 3s 163ms/step - loss: 0.2787 - acc: 0.8381 - dice_coef: 0.8374 - jaccard_coef: 0.7213 - val_loss: 0.3361 - val_acc: 0.7982 - val_dice_coef: 0.7979 - val_jaccard_coef: 0.6639 - lr: 0.0010\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2790 - acc: 0.8378 - dice_coef: 0.8373 - jaccard_coef: 0.7210 - val_loss: 0.3133 - val_acc: 0.8147 - val_dice_coef: 0.8141 - val_jaccard_coef: 0.6867 - lr: 0.0010\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2901 - acc: 0.8301 - dice_coef: 0.8294 - jaccard_coef: 0.7099 - val_loss: 0.3965 - val_acc: 0.7556 - val_dice_coef: 0.7524 - val_jaccard_coef: 0.6035 - lr: 0.0010\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2810 - acc: 0.8365 - dice_coef: 0.8359 - jaccard_coef: 0.7190 - val_loss: 0.3298 - val_acc: 0.8049 - val_dice_coef: 0.8025 - val_jaccard_coef: 0.6702 - lr: 0.0010\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2778 - acc: 0.8385 - dice_coef: 0.8381 - jaccard_coef: 0.7222 - val_loss: 0.3161 - val_acc: 0.8128 - val_dice_coef: 0.8122 - val_jaccard_coef: 0.6839 - lr: 0.0010\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2740 - acc: 0.8413 - dice_coef: 0.8408 - jaccard_coef: 0.7260\n",
            "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2740 - acc: 0.8413 - dice_coef: 0.8408 - jaccard_coef: 0.7260 - val_loss: 0.3021 - val_acc: 0.8225 - val_dice_coef: 0.8220 - val_jaccard_coef: 0.6979 - lr: 0.0010\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 4s 229ms/step - loss: 0.2695 - acc: 0.8441 - dice_coef: 0.8437 - jaccard_coef: 0.7305 - val_loss: 0.2985 - val_acc: 0.8250 - val_dice_coef: 0.8245 - val_jaccard_coef: 0.7015 - lr: 3.0000e-04\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2673 - acc: 0.8455 - dice_coef: 0.8450 - jaccard_coef: 0.7327 - val_loss: 0.3000 - val_acc: 0.8239 - val_dice_coef: 0.8235 - val_jaccard_coef: 0.7000 - lr: 3.0000e-04\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 3s 163ms/step - loss: 0.2670 - acc: 0.8457 - dice_coef: 0.8452 - jaccard_coef: 0.7330 - val_loss: 0.3008 - val_acc: 0.8234 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 3.0000e-04\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2662 - acc: 0.8466 - dice_coef: 0.8462 - jaccard_coef: 0.7338 - val_loss: 0.3008 - val_acc: 0.8235 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6992 - lr: 3.0000e-04\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2652 - acc: 0.8471 - dice_coef: 0.8466 - jaccard_coef: 0.7348 - val_loss: 0.3013 - val_acc: 0.8232 - val_dice_coef: 0.8226 - val_jaccard_coef: 0.6987 - lr: 3.0000e-04\n",
            "Epoch 132/200\n",
            "18/18 [==============================] - 3s 159ms/step - loss: 0.2649 - acc: 0.8476 - dice_coef: 0.8471 - jaccard_coef: 0.7351 - val_loss: 0.3012 - val_acc: 0.8233 - val_dice_coef: 0.8227 - val_jaccard_coef: 0.6988 - lr: 3.0000e-04\n",
            "Epoch 133/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2648 - acc: 0.8472 - dice_coef: 0.8467 - jaccard_coef: 0.7352 - val_loss: 0.3045 - val_acc: 0.8210 - val_dice_coef: 0.8203 - val_jaccard_coef: 0.6955 - lr: 3.0000e-04\n",
            "Epoch 134/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2646 - acc: 0.8476 - dice_coef: 0.8471 - jaccard_coef: 0.7354 - val_loss: 0.3020 - val_acc: 0.8227 - val_dice_coef: 0.8221 - val_jaccard_coef: 0.6980 - lr: 3.0000e-04\n",
            "Epoch 135/200\n",
            "18/18 [==============================] - 3s 163ms/step - loss: 0.2639 - acc: 0.8480 - dice_coef: 0.8475 - jaccard_coef: 0.7361 - val_loss: 0.3014 - val_acc: 0.8231 - val_dice_coef: 0.8225 - val_jaccard_coef: 0.6986 - lr: 3.0000e-04\n",
            "Epoch 136/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2631 - acc: 0.8479 - dice_coef: 0.8474 - jaccard_coef: 0.7369 - val_loss: 0.3060 - val_acc: 0.8198 - val_dice_coef: 0.8193 - val_jaccard_coef: 0.6940 - lr: 3.0000e-04\n",
            "Epoch 137/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2635 - acc: 0.8484 - dice_coef: 0.8479 - jaccard_coef: 0.7365 - val_loss: 0.3044 - val_acc: 0.8209 - val_dice_coef: 0.8204 - val_jaccard_coef: 0.6956 - lr: 3.0000e-04\n",
            "Epoch 138/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2630 - acc: 0.8486 - dice_coef: 0.8481 - jaccard_coef: 0.7370 - val_loss: 0.3054 - val_acc: 0.8203 - val_dice_coef: 0.8197 - val_jaccard_coef: 0.6946 - lr: 3.0000e-04\n",
            "Epoch 139/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2620 - acc: 0.8491 - dice_coef: 0.8485 - jaccard_coef: 0.7380 - val_loss: 0.3032 - val_acc: 0.8218 - val_dice_coef: 0.8212 - val_jaccard_coef: 0.6968 - lr: 3.0000e-04\n",
            "Epoch 140/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2624 - acc: 0.8491 - dice_coef: 0.8486 - jaccard_coef: 0.7376 - val_loss: 0.3007 - val_acc: 0.8236 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6993 - lr: 3.0000e-04\n",
            "Epoch 141/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2612 - acc: 0.8494 - dice_coef: 0.8489 - jaccard_coef: 0.7388 - val_loss: 0.3048 - val_acc: 0.8208 - val_dice_coef: 0.8201 - val_jaccard_coef: 0.6952 - lr: 3.0000e-04\n",
            "Epoch 142/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2618 - acc: 0.8493 - dice_coef: 0.8487 - jaccard_coef: 0.7382 - val_loss: 0.2998 - val_acc: 0.8242 - val_dice_coef: 0.8236 - val_jaccard_coef: 0.7002 - lr: 3.0000e-04\n",
            "Epoch 143/200\n",
            "18/18 [==============================] - 4s 221ms/step - loss: 0.2611 - acc: 0.8496 - dice_coef: 0.8491 - jaccard_coef: 0.7389 - val_loss: 0.2980 - val_acc: 0.8255 - val_dice_coef: 0.8249 - val_jaccard_coef: 0.7020 - lr: 3.0000e-04\n",
            "Epoch 144/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2604 - acc: 0.8497 - dice_coef: 0.8492 - jaccard_coef: 0.7396 - val_loss: 0.3010 - val_acc: 0.8233 - val_dice_coef: 0.8228 - val_jaccard_coef: 0.6990 - lr: 3.0000e-04\n",
            "Epoch 145/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2606 - acc: 0.8497 - dice_coef: 0.8492 - jaccard_coef: 0.7394 - val_loss: 0.3045 - val_acc: 0.8208 - val_dice_coef: 0.8204 - val_jaccard_coef: 0.6955 - lr: 3.0000e-04\n",
            "Epoch 146/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2609 - acc: 0.8500 - dice_coef: 0.8495 - jaccard_coef: 0.7391 - val_loss: 0.3019 - val_acc: 0.8229 - val_dice_coef: 0.8222 - val_jaccard_coef: 0.6981 - lr: 3.0000e-04\n",
            "Epoch 147/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2613 - acc: 0.8499 - dice_coef: 0.8493 - jaccard_coef: 0.7387 - val_loss: 0.3019 - val_acc: 0.8228 - val_dice_coef: 0.8222 - val_jaccard_coef: 0.6981 - lr: 3.0000e-04\n",
            "Epoch 148/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2610 - acc: 0.8500 - dice_coef: 0.8495 - jaccard_coef: 0.7390 - val_loss: 0.3019 - val_acc: 0.8227 - val_dice_coef: 0.8222 - val_jaccard_coef: 0.6981 - lr: 3.0000e-04\n",
            "Epoch 149/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2602 - acc: 0.8505 - dice_coef: 0.8499 - jaccard_coef: 0.7398 - val_loss: 0.3015 - val_acc: 0.8230 - val_dice_coef: 0.8224 - val_jaccard_coef: 0.6985 - lr: 3.0000e-04\n",
            "Epoch 150/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2603 - acc: 0.8506 - dice_coef: 0.8501 - jaccard_coef: 0.7397 - val_loss: 0.3036 - val_acc: 0.8213 - val_dice_coef: 0.8209 - val_jaccard_coef: 0.6964 - lr: 3.0000e-04\n",
            "Epoch 151/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2594 - acc: 0.8507 - dice_coef: 0.8502 - jaccard_coef: 0.7406 - val_loss: 0.3016 - val_acc: 0.8229 - val_dice_coef: 0.8224 - val_jaccard_coef: 0.6984 - lr: 3.0000e-04\n",
            "Epoch 152/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2587 - acc: 0.8510 - dice_coef: 0.8505 - jaccard_coef: 0.7413 - val_loss: 0.3002 - val_acc: 0.8238 - val_dice_coef: 0.8233 - val_jaccard_coef: 0.6998 - lr: 3.0000e-04\n",
            "Epoch 153/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2586 - acc: 0.8512 - dice_coef: 0.8507 - jaccard_coef: 0.7414 - val_loss: 0.3018 - val_acc: 0.8226 - val_dice_coef: 0.8222 - val_jaccard_coef: 0.6982 - lr: 3.0000e-04\n",
            "Epoch 154/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2587 - acc: 0.8512 - dice_coef: 0.8506 - jaccard_coef: 0.7413 - val_loss: 0.3026 - val_acc: 0.8222 - val_dice_coef: 0.8217 - val_jaccard_coef: 0.6974 - lr: 3.0000e-04\n",
            "Epoch 155/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2587 - acc: 0.8513 - dice_coef: 0.8508 - jaccard_coef: 0.7413 - val_loss: 0.3026 - val_acc: 0.8222 - val_dice_coef: 0.8217 - val_jaccard_coef: 0.6974 - lr: 3.0000e-04\n",
            "Epoch 156/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2583 - acc: 0.8514 - dice_coef: 0.8509 - jaccard_coef: 0.7417 - val_loss: 0.3037 - val_acc: 0.8214 - val_dice_coef: 0.8209 - val_jaccard_coef: 0.6963 - lr: 3.0000e-04\n",
            "Epoch 157/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2582 - acc: 0.8514 - dice_coef: 0.8509 - jaccard_coef: 0.7418 - val_loss: 0.3029 - val_acc: 0.8220 - val_dice_coef: 0.8215 - val_jaccard_coef: 0.6971 - lr: 3.0000e-04\n",
            "Epoch 158/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2580 - acc: 0.8515 - dice_coef: 0.8510 - jaccard_coef: 0.7420 - val_loss: 0.3038 - val_acc: 0.8213 - val_dice_coef: 0.8208 - val_jaccard_coef: 0.6962 - lr: 3.0000e-04\n",
            "Epoch 159/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2576 - acc: 0.8516 - dice_coef: 0.8511 - jaccard_coef: 0.7424 - val_loss: 0.3022 - val_acc: 0.8225 - val_dice_coef: 0.8220 - val_jaccard_coef: 0.6978 - lr: 3.0000e-04\n",
            "Epoch 160/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2575 - acc: 0.8519 - dice_coef: 0.8514 - jaccard_coef: 0.7425 - val_loss: 0.3017 - val_acc: 0.8229 - val_dice_coef: 0.8223 - val_jaccard_coef: 0.6983 - lr: 3.0000e-04\n",
            "Epoch 161/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2580 - acc: 0.8519 - dice_coef: 0.8514 - jaccard_coef: 0.7420 - val_loss: 0.2999 - val_acc: 0.8240 - val_dice_coef: 0.8235 - val_jaccard_coef: 0.7001 - lr: 3.0000e-04\n",
            "Epoch 162/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2576 - acc: 0.8519 - dice_coef: 0.8513 - jaccard_coef: 0.7424 - val_loss: 0.3021 - val_acc: 0.8224 - val_dice_coef: 0.8220 - val_jaccard_coef: 0.6979 - lr: 3.0000e-04\n",
            "Epoch 163/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2575 - acc: 0.8519 - dice_coef: 0.8514 - jaccard_coef: 0.7425\n",
            "Epoch 00163: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2575 - acc: 0.8519 - dice_coef: 0.8514 - jaccard_coef: 0.7425 - val_loss: 0.3051 - val_acc: 0.8204 - val_dice_coef: 0.8199 - val_jaccard_coef: 0.6949 - lr: 3.0000e-04\n",
            "Epoch 164/200\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.2567 - acc: 0.8526 - dice_coef: 0.8521 - jaccard_coef: 0.7433 - val_loss: 0.3043 - val_acc: 0.8211 - val_dice_coef: 0.8205 - val_jaccard_coef: 0.6957 - lr: 9.0000e-05\n",
            "Epoch 165/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2562 - acc: 0.8531 - dice_coef: 0.8526 - jaccard_coef: 0.7438 - val_loss: 0.3014 - val_acc: 0.8230 - val_dice_coef: 0.8225 - val_jaccard_coef: 0.6986 - lr: 9.0000e-05\n",
            "Epoch 166/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2555 - acc: 0.8531 - dice_coef: 0.8526 - jaccard_coef: 0.7445 - val_loss: 0.3014 - val_acc: 0.8231 - val_dice_coef: 0.8225 - val_jaccard_coef: 0.6986 - lr: 9.0000e-05\n",
            "Epoch 167/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2557 - acc: 0.8534 - dice_coef: 0.8529 - jaccard_coef: 0.7443 - val_loss: 0.3008 - val_acc: 0.8234 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 9.0000e-05\n",
            "Epoch 168/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2557 - acc: 0.8534 - dice_coef: 0.8528 - jaccard_coef: 0.7443 - val_loss: 0.3007 - val_acc: 0.8233 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6993 - lr: 9.0000e-05\n",
            "Epoch 169/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2557 - acc: 0.8533 - dice_coef: 0.8528 - jaccard_coef: 0.7443 - val_loss: 0.3004 - val_acc: 0.8236 - val_dice_coef: 0.8232 - val_jaccard_coef: 0.6996 - lr: 9.0000e-05\n",
            "Epoch 170/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2552 - acc: 0.8537 - dice_coef: 0.8531 - jaccard_coef: 0.7448 - val_loss: 0.2999 - val_acc: 0.8240 - val_dice_coef: 0.8235 - val_jaccard_coef: 0.7001 - lr: 9.0000e-05\n",
            "Epoch 171/200\n",
            "18/18 [==============================] - 3s 165ms/step - loss: 0.2548 - acc: 0.8535 - dice_coef: 0.8530 - jaccard_coef: 0.7452 - val_loss: 0.3017 - val_acc: 0.8227 - val_dice_coef: 0.8223 - val_jaccard_coef: 0.6983 - lr: 9.0000e-05\n",
            "Epoch 172/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2556 - acc: 0.8535 - dice_coef: 0.8530 - jaccard_coef: 0.7444 - val_loss: 0.3006 - val_acc: 0.8235 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6994 - lr: 9.0000e-05\n",
            "Epoch 173/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2556 - acc: 0.8534 - dice_coef: 0.8528 - jaccard_coef: 0.7444 - val_loss: 0.3008 - val_acc: 0.8233 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 9.0000e-05\n",
            "Epoch 174/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2558 - acc: 0.8535 - dice_coef: 0.8530 - jaccard_coef: 0.7442 - val_loss: 0.3005 - val_acc: 0.8236 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6995 - lr: 9.0000e-05\n",
            "Epoch 175/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2551 - acc: 0.8536 - dice_coef: 0.8531 - jaccard_coef: 0.7449 - val_loss: 0.3010 - val_acc: 0.8233 - val_dice_coef: 0.8228 - val_jaccard_coef: 0.6990 - lr: 9.0000e-05\n",
            "Epoch 176/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2554 - acc: 0.8536 - dice_coef: 0.8530 - jaccard_coef: 0.7446 - val_loss: 0.3005 - val_acc: 0.8236 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6995 - lr: 9.0000e-05\n",
            "Epoch 177/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2546 - acc: 0.8539 - dice_coef: 0.8534 - jaccard_coef: 0.7454 - val_loss: 0.3006 - val_acc: 0.8236 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6994 - lr: 9.0000e-05\n",
            "Epoch 178/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2543 - acc: 0.8538 - dice_coef: 0.8532 - jaccard_coef: 0.7457 - val_loss: 0.3008 - val_acc: 0.8233 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 9.0000e-05\n",
            "Epoch 179/200\n",
            "18/18 [==============================] - 3s 163ms/step - loss: 0.2543 - acc: 0.8539 - dice_coef: 0.8534 - jaccard_coef: 0.7457 - val_loss: 0.3011 - val_acc: 0.8232 - val_dice_coef: 0.8227 - val_jaccard_coef: 0.6989 - lr: 9.0000e-05\n",
            "Epoch 180/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2543 - acc: 0.8539 - dice_coef: 0.8533 - jaccard_coef: 0.7457 - val_loss: 0.3006 - val_acc: 0.8233 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6994 - lr: 9.0000e-05\n",
            "Epoch 181/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2546 - acc: 0.8539 - dice_coef: 0.8533 - jaccard_coef: 0.7454 - val_loss: 0.3005 - val_acc: 0.8235 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6995 - lr: 9.0000e-05\n",
            "Epoch 182/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2542 - acc: 0.8540 - dice_coef: 0.8534 - jaccard_coef: 0.7458 - val_loss: 0.3013 - val_acc: 0.8230 - val_dice_coef: 0.8226 - val_jaccard_coef: 0.6987 - lr: 9.0000e-05\n",
            "Epoch 183/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.2544 - acc: 0.8539 - dice_coef: 0.8533 - jaccard_coef: 0.7456\n",
            "Epoch 00183: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2544 - acc: 0.8539 - dice_coef: 0.8533 - jaccard_coef: 0.7456 - val_loss: 0.3013 - val_acc: 0.8231 - val_dice_coef: 0.8226 - val_jaccard_coef: 0.6987 - lr: 9.0000e-05\n",
            "Epoch 184/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2545 - acc: 0.8542 - dice_coef: 0.8536 - jaccard_coef: 0.7455 - val_loss: 0.3010 - val_acc: 0.8232 - val_dice_coef: 0.8228 - val_jaccard_coef: 0.6990 - lr: 2.7000e-05\n",
            "Epoch 185/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2544 - acc: 0.8543 - dice_coef: 0.8537 - jaccard_coef: 0.7456 - val_loss: 0.3008 - val_acc: 0.8234 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 2.7000e-05\n",
            "Epoch 186/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2540 - acc: 0.8541 - dice_coef: 0.8535 - jaccard_coef: 0.7460 - val_loss: 0.3008 - val_acc: 0.8234 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 2.7000e-05\n",
            "Epoch 187/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2548 - acc: 0.8541 - dice_coef: 0.8535 - jaccard_coef: 0.7452 - val_loss: 0.3010 - val_acc: 0.8232 - val_dice_coef: 0.8228 - val_jaccard_coef: 0.6990 - lr: 2.7000e-05\n",
            "Epoch 188/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2543 - acc: 0.8539 - dice_coef: 0.8533 - jaccard_coef: 0.7457 - val_loss: 0.3006 - val_acc: 0.8234 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6994 - lr: 2.7000e-05\n",
            "Epoch 189/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2542 - acc: 0.8543 - dice_coef: 0.8537 - jaccard_coef: 0.7458 - val_loss: 0.3004 - val_acc: 0.8236 - val_dice_coef: 0.8232 - val_jaccard_coef: 0.6996 - lr: 2.7000e-05\n",
            "Epoch 190/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2549 - acc: 0.8543 - dice_coef: 0.8537 - jaccard_coef: 0.7451 - val_loss: 0.3003 - val_acc: 0.8238 - val_dice_coef: 0.8233 - val_jaccard_coef: 0.6997 - lr: 2.7000e-05\n",
            "Epoch 191/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2543 - acc: 0.8543 - dice_coef: 0.8538 - jaccard_coef: 0.7457 - val_loss: 0.3006 - val_acc: 0.8236 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6994 - lr: 2.7000e-05\n",
            "Epoch 192/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2541 - acc: 0.8543 - dice_coef: 0.8538 - jaccard_coef: 0.7459 - val_loss: 0.3007 - val_acc: 0.8236 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6993 - lr: 2.7000e-05\n",
            "Epoch 193/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2546 - acc: 0.8543 - dice_coef: 0.8537 - jaccard_coef: 0.7454 - val_loss: 0.3007 - val_acc: 0.8234 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6993 - lr: 2.7000e-05\n",
            "Epoch 194/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2544 - acc: 0.8544 - dice_coef: 0.8538 - jaccard_coef: 0.7456 - val_loss: 0.3009 - val_acc: 0.8233 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6991 - lr: 2.7000e-05\n",
            "Epoch 195/200\n",
            "18/18 [==============================] - 3s 163ms/step - loss: 0.2537 - acc: 0.8545 - dice_coef: 0.8539 - jaccard_coef: 0.7463 - val_loss: 0.3006 - val_acc: 0.8234 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6994 - lr: 2.7000e-05\n",
            "Epoch 196/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2542 - acc: 0.8544 - dice_coef: 0.8538 - jaccard_coef: 0.7458 - val_loss: 0.3006 - val_acc: 0.8235 - val_dice_coef: 0.8230 - val_jaccard_coef: 0.6994 - lr: 2.7000e-05\n",
            "Epoch 197/200\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2532 - acc: 0.8546 - dice_coef: 0.8540 - jaccard_coef: 0.7468 - val_loss: 0.3009 - val_acc: 0.8233 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6991 - lr: 2.7000e-05\n",
            "Epoch 198/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2536 - acc: 0.8546 - dice_coef: 0.8540 - jaccard_coef: 0.7464 - val_loss: 0.3008 - val_acc: 0.8233 - val_dice_coef: 0.8229 - val_jaccard_coef: 0.6992 - lr: 2.7000e-05\n",
            "Epoch 199/200\n",
            "18/18 [==============================] - 3s 162ms/step - loss: 0.2543 - acc: 0.8544 - dice_coef: 0.8538 - jaccard_coef: 0.7457 - val_loss: 0.3005 - val_acc: 0.8236 - val_dice_coef: 0.8231 - val_jaccard_coef: 0.6995 - lr: 2.7000e-05\n",
            "Epoch 200/200\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.2535 - acc: 0.8545 - dice_coef: 0.8539 - jaccard_coef: 0.7465 - val_loss: 0.3002 - val_acc: 0.8238 - val_dice_coef: 0.8234 - val_jaccard_coef: 0.6998 - lr: 2.7000e-05\n",
            "Run time for UNet [s]:  615.0235579013824\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "model_UNet.compile(optimizer=\"adam\", loss = jaccard_coef_loss, metrics=[\"acc\", dice_coef , jaccard_coef])\n",
        "\n",
        "callbacks_UNet = [\n",
        "    ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                         patience =20,\n",
        "                         verbose = 1,\n",
        "                         factor = 0.3,\n",
        "                         min_lr = 0.00001),\n",
        "    ModelCheckpoint(filepath=r'model_UNet-tgs-salt.h5', verbose=0, save_best_only=True, save_weights_only=False)]\n",
        "\n",
        "start_UNet = time.time()\n",
        "\n",
        "results_UNet =model_UNet.fit(X1, y_train_cat, batch_size=8, epochs=200, validation_data=(X_test, y_test_cat),callbacks=callbacks_UNet)\n",
        "\n",
        "end_UNet = time.time()\n",
        "print(\"Run time for UNet [s]: \",end_UNet-start_UNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a6b47186",
      "metadata": {
        "id": "a6b47186"
      },
      "outputs": [],
      "source": [
        "#model_UNet.save('unet.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f9178e40",
      "metadata": {
        "id": "f9178e40"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('unet.pickle' , 'wb') as file:\n",
        "  pickle.dump(results_UNet.history,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e023db83",
      "metadata": {
        "id": "e023db83"
      },
      "outputs": [],
      "source": [
        "pred_train = model_UNet.predict(X1)\n",
        "pred_val = model_UNet.predict(X_test)\n",
        "pred_test = model_UNet.predict(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c2e4dd1e",
      "metadata": {
        "id": "c2e4dd1e"
      },
      "outputs": [],
      "source": [
        "pred_train1 = np.argmax(pred_train, axis = 3)\n",
        "pred_val1 = np.argmax(pred_val, axis = 3)\n",
        "pred_test1 = np.argmax(pred_test, axis = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ee10c785",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee10c785",
        "outputId": "cf088c63-0b1f-43b4-97eb-bc8d26c0f42f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47140238"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(y_train_cat, pred_train)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0c4a984b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c4a984b",
        "outputId": "bd10c093-a7b3-452e-d952-f34798a716e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5090857"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(y_test_cat, pred_val)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3d03d7ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d03d7ba",
        "outputId": "93862f02-308b-4fec-fb32-7d5adee2f965"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48454654"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(test_masks_cat, pred_test)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56dbe3f",
      "metadata": {
        "id": "f56dbe3f"
      },
      "outputs": [],
      "source": [
        "#==========================================================================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8167f379",
      "metadata": {
        "id": "8167f379"
      },
      "outputs": [],
      "source": [
        "#PSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "94794191",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94794191",
        "outputId": "170aa434-05dc-4dea-8f2a-a8c9dac0c02f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "inputs = Input((128, 128, 3))\n",
        "s = inputs\n",
        "resnet = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=s)\n",
        "x = resnet.get_layer(\"conv2_block3_2_relu\").output\n",
        "x = Conv2D(filters=64,kernel_size=(3, 3), dilation_rate= 3 ,activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "x = Conv2D(filters=128 ,kernel_size=(3, 3), dilation_rate= 3 ,activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "\n",
        "#base Pooling\n",
        "base = GlobalAveragePooling2D()(x)\n",
        "reshaped = tf.keras.layers.Reshape((1,1,128))(base)\n",
        "\n",
        "#first conv\n",
        "first_conv = Conv2D(filters=32,kernel_size=(1,1), activation='relu', kernel_initializer='he_normal', padding='same')(reshaped)\n",
        "first_up = UpSampling2D(size=32,interpolation='bilinear')(first_conv)\n",
        "\n",
        "\n",
        "#second conv\n",
        "second = AveragePooling2D(pool_size=(2,2))(x)\n",
        "second_conv = Conv2D(filters=32,kernel_size=(1,1), activation='relu', kernel_initializer='he_normal', padding='same')(second)\n",
        "second_up = UpSampling2D(size=2,interpolation='bilinear')(second_conv)\n",
        "\n",
        "\n",
        "#third conv\n",
        "third = AveragePooling2D(pool_size=(4,4))(x)\n",
        "third_conv = Conv2D(filters=32,kernel_size=(1,1), activation='relu', kernel_initializer='he_normal', padding='same')(third)\n",
        "third_up = UpSampling2D(size=4,interpolation='bilinear')(third_conv)\n",
        "\n",
        "#fourth\n",
        "fourth = AveragePooling2D(pool_size=(8,8))(x)\n",
        "fourth_conv = Conv2D(filters=32,kernel_size=(1,1), activation='relu', kernel_initializer='he_normal', padding='same')(fourth)\n",
        "fourth_up = UpSampling2D(size=8,interpolation='bilinear')(fourth_conv)\n",
        "\n",
        "concated = tf.keras.layers.concatenate([x,first_up,second_up,third_up,fourth_up])\n",
        "X = Conv2DTranspose(128, (2, 2), strides=2, padding=\"same\")(concated)\n",
        "\n",
        "X = Conv2D(filters=64,kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(X)\n",
        "X = Dropout(0.1)(X)\n",
        "X = BatchNormalization()(X)\n",
        "\n",
        "X = Conv2DTranspose(64, (2, 2), strides=2, padding=\"same\")(X)\n",
        "\n",
        "X = Conv2D(filters=32,kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(X)\n",
        "X = Dropout(0.1)(X)\n",
        "X = BatchNormalization()(X)\n",
        "\n",
        "outputs = Conv2D(31, (1, 1), activation='softmax')(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f3facadb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3facadb",
        "outputId": "347d7feb-62fd-451c-a928-9de27d9d6734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 134, 134, 3)  0           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 64, 64, 64)   9472        ['conv1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 64, 64, 64)   256         ['conv1_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 64, 64, 64)   0           ['conv1_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 66, 66, 64)   0           ['conv1_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 32, 32, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 32, 32, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 32, 32, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 32, 32, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 32, 32, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 32, 32, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 32, 32, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 32, 32, 64)   36928       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 32, 32, 64)   0           ['conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 32, 32, 128)  73856       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32, 32, 128)  0           ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 128)         0           ['dropout_1[0][0]']              \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 1, 1, 128)    0           ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 128)  0          ['dropout_1[0][0]']              \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 8, 8, 128)   0           ['dropout_1[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 4, 4, 128)   0           ['dropout_1[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 1, 1, 32)     4128        ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 32)   4128        ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 8, 8, 32)     4128        ['average_pooling2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 4, 4, 32)     4128        ['average_pooling2d_2[0][0]']    \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 32, 32, 32)   0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 32)  0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 32)  0           ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 32, 32, 32)  0           ['conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 32, 256)  0           ['dropout_1[0][0]',              \n",
            "                                                                  'up_sampling2d[0][0]',          \n",
            "                                                                  'up_sampling2d_1[0][0]',        \n",
            "                                                                  'up_sampling2d_2[0][0]',        \n",
            "                                                                  'up_sampling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2DTran  (None, 64, 64, 128)  131200     ['concatenate_4[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 64, 64, 64)   73792       ['conv2d_transpose_4[0][0]']     \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64, 64, 64)   0           ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 64, 64, 64)  256         ['dropout_2[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, 128, 128, 64  16448      ['batch_normalization_18[0][0]'] \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 128, 128, 32  18464       ['conv2d_transpose_5[0][0]']     \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 128, 128, 32  0           ['conv2d_26[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 128, 128, 32  128        ['dropout_3[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 128, 128, 31  1023        ['batch_normalization_19[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 580,703\n",
            "Trainable params: 578,079\n",
            "Non-trainable params: 2,624\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_psp = Model(inputs=[inputs], outputs=[outputs])\n",
        "model_psp.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3cc988eb",
      "metadata": {
        "id": "3cc988eb"
      },
      "outputs": [],
      "source": [
        "model_psp.compile(optimizer='adam', \n",
        "              loss= dice_coef_loss,\n",
        "              metrics=[jaccard_coef,dice_coef,'accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "UngPcP_xOr1T",
      "metadata": {
        "id": "UngPcP_xOr1T"
      },
      "outputs": [],
      "source": [
        "lrd = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                         patience =20,\n",
        "                         verbose = 1,\n",
        "                         factor = 0.3,\n",
        "                         min_lr = 0.00001)\n",
        "\n",
        "mcp = ModelCheckpoint(filepath=r\"model.h5\", \n",
        "    save_weights_only=True,\n",
        "    monitor='val_jaccard_coef',\n",
        "    mode='auto',\n",
        "    verbose=0,\n",
        "    save_best_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d252f6f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d252f6f2",
        "outputId": "464fbb4a-d7b1-48de-b369-37131ccd6d23",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "18/18 [==============================] - 4s 74ms/step - loss: 0.8763 - jaccard_coef: 0.0667 - dice_coef: 0.1237 - accuracy: 0.1862 - val_loss: 0.8990 - val_jaccard_coef: 0.0532 - val_dice_coef: 0.1010 - val_accuracy: 0.1496 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.7089 - jaccard_coef: 0.1714 - dice_coef: 0.2911 - accuracy: 0.3615 - val_loss: 0.8844 - val_jaccard_coef: 0.0613 - val_dice_coef: 0.1156 - val_accuracy: 0.1326 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.6310 - jaccard_coef: 0.2266 - dice_coef: 0.3690 - accuracy: 0.4132 - val_loss: 0.8416 - val_jaccard_coef: 0.0860 - val_dice_coef: 0.1584 - val_accuracy: 0.1689 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 1s 46ms/step - loss: 0.6028 - jaccard_coef: 0.2481 - dice_coef: 0.3972 - accuracy: 0.4265 - val_loss: 0.8467 - val_jaccard_coef: 0.0830 - val_dice_coef: 0.1533 - val_accuracy: 0.1655 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.5882 - jaccard_coef: 0.2596 - dice_coef: 0.4118 - accuracy: 0.4369 - val_loss: 0.8563 - val_jaccard_coef: 0.0774 - val_dice_coef: 0.1437 - val_accuracy: 0.1496 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.5695 - jaccard_coef: 0.2746 - dice_coef: 0.4305 - accuracy: 0.4506 - val_loss: 0.8473 - val_jaccard_coef: 0.0827 - val_dice_coef: 0.1527 - val_accuracy: 0.1606 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.4646 - jaccard_coef: 0.3678 - dice_coef: 0.5354 - accuracy: 0.5867 - val_loss: 0.8219 - val_jaccard_coef: 0.0978 - val_dice_coef: 0.1781 - val_accuracy: 0.1919 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.3995 - jaccard_coef: 0.4304 - dice_coef: 0.6005 - accuracy: 0.6257 - val_loss: 0.8504 - val_jaccard_coef: 0.0809 - val_dice_coef: 0.1496 - val_accuracy: 0.1520 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.3850 - jaccard_coef: 0.4447 - dice_coef: 0.6150 - accuracy: 0.6308 - val_loss: 0.8321 - val_jaccard_coef: 0.0917 - val_dice_coef: 0.1679 - val_accuracy: 0.1750 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.3726 - jaccard_coef: 0.4579 - dice_coef: 0.6274 - accuracy: 0.6391 - val_loss: 0.8218 - val_jaccard_coef: 0.0979 - val_dice_coef: 0.1782 - val_accuracy: 0.1819 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.3325 - jaccard_coef: 0.5033 - dice_coef: 0.6675 - accuracy: 0.6839 - val_loss: 0.8540 - val_jaccard_coef: 0.0788 - val_dice_coef: 0.1460 - val_accuracy: 0.1444 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 1s 46ms/step - loss: 0.2635 - jaccard_coef: 0.5854 - dice_coef: 0.7365 - accuracy: 0.7540 - val_loss: 0.7953 - val_jaccard_coef: 0.1142 - val_dice_coef: 0.2047 - val_accuracy: 0.2008 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 1s 46ms/step - loss: 0.2330 - jaccard_coef: 0.6229 - dice_coef: 0.7670 - accuracy: 0.7802 - val_loss: 0.7942 - val_jaccard_coef: 0.1150 - val_dice_coef: 0.2058 - val_accuracy: 0.2384 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.2269 - jaccard_coef: 0.6312 - dice_coef: 0.7731 - accuracy: 0.7854 - val_loss: 0.8564 - val_jaccard_coef: 0.0774 - val_dice_coef: 0.1436 - val_accuracy: 0.1433 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.2163 - jaccard_coef: 0.6452 - dice_coef: 0.7837 - accuracy: 0.7926 - val_loss: 0.8431 - val_jaccard_coef: 0.0853 - val_dice_coef: 0.1569 - val_accuracy: 0.1590 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.2113 - jaccard_coef: 0.6522 - dice_coef: 0.7887 - accuracy: 0.7958 - val_loss: 0.8454 - val_jaccard_coef: 0.0839 - val_dice_coef: 0.1546 - val_accuracy: 0.1533 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.2100 - jaccard_coef: 0.6536 - dice_coef: 0.7900 - accuracy: 0.7963 - val_loss: 0.8579 - val_jaccard_coef: 0.0765 - val_dice_coef: 0.1421 - val_accuracy: 0.1418 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.2015 - jaccard_coef: 0.6659 - dice_coef: 0.7985 - accuracy: 0.8041 - val_loss: 0.8141 - val_jaccard_coef: 0.1025 - val_dice_coef: 0.1859 - val_accuracy: 0.1868 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1962 - jaccard_coef: 0.6731 - dice_coef: 0.8038 - accuracy: 0.8087 - val_loss: 0.8194 - val_jaccard_coef: 0.0993 - val_dice_coef: 0.1806 - val_accuracy: 0.1847 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1977 - jaccard_coef: 0.6706 - dice_coef: 0.8023 - accuracy: 0.8071 - val_loss: 0.8114 - val_jaccard_coef: 0.1042 - val_dice_coef: 0.1886 - val_accuracy: 0.1869 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1958 - jaccard_coef: 0.6740 - dice_coef: 0.8042 - accuracy: 0.8084 - val_loss: 0.8206 - val_jaccard_coef: 0.0986 - val_dice_coef: 0.1794 - val_accuracy: 0.1826 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1919 - jaccard_coef: 0.6793 - dice_coef: 0.8081 - accuracy: 0.8121 - val_loss: 0.8341 - val_jaccard_coef: 0.0905 - val_dice_coef: 0.1659 - val_accuracy: 0.1685 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1891 - jaccard_coef: 0.6824 - dice_coef: 0.8109 - accuracy: 0.8141 - val_loss: 0.8592 - val_jaccard_coef: 0.0758 - val_dice_coef: 0.1408 - val_accuracy: 0.1414 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1856 - jaccard_coef: 0.6879 - dice_coef: 0.8144 - accuracy: 0.8175 - val_loss: 0.8607 - val_jaccard_coef: 0.0749 - val_dice_coef: 0.1393 - val_accuracy: 0.1376 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1893 - jaccard_coef: 0.6826 - dice_coef: 0.8107 - accuracy: 0.8137 - val_loss: 0.7051 - val_jaccard_coef: 0.1730 - val_dice_coef: 0.2949 - val_accuracy: 0.2979 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 1s 46ms/step - loss: 0.1875 - jaccard_coef: 0.6848 - dice_coef: 0.8125 - accuracy: 0.8155 - val_loss: 0.7862 - val_jaccard_coef: 0.1197 - val_dice_coef: 0.2138 - val_accuracy: 0.2166 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1878 - jaccard_coef: 0.6851 - dice_coef: 0.8122 - accuracy: 0.8152 - val_loss: 0.8073 - val_jaccard_coef: 0.1068 - val_dice_coef: 0.1927 - val_accuracy: 0.1978 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1794 - jaccard_coef: 0.6973 - dice_coef: 0.8206 - accuracy: 0.8232 - val_loss: 0.8264 - val_jaccard_coef: 0.0951 - val_dice_coef: 0.1736 - val_accuracy: 0.1733 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1760 - jaccard_coef: 0.7023 - dice_coef: 0.8240 - accuracy: 0.8263 - val_loss: 0.8275 - val_jaccard_coef: 0.0945 - val_dice_coef: 0.1725 - val_accuracy: 0.1722 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1768 - jaccard_coef: 0.7003 - dice_coef: 0.8232 - accuracy: 0.8255 - val_loss: 0.8189 - val_jaccard_coef: 0.0997 - val_dice_coef: 0.1811 - val_accuracy: 0.1837 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1729 - jaccard_coef: 0.7061 - dice_coef: 0.8271 - accuracy: 0.8292 - val_loss: 0.8117 - val_jaccard_coef: 0.1040 - val_dice_coef: 0.1883 - val_accuracy: 0.1939 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1720 - jaccard_coef: 0.7074 - dice_coef: 0.8280 - accuracy: 0.8299 - val_loss: 0.8056 - val_jaccard_coef: 0.1077 - val_dice_coef: 0.1944 - val_accuracy: 0.1984 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1716 - jaccard_coef: 0.7086 - dice_coef: 0.8284 - accuracy: 0.8303 - val_loss: 0.8243 - val_jaccard_coef: 0.0963 - val_dice_coef: 0.1757 - val_accuracy: 0.1741 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1705 - jaccard_coef: 0.7096 - dice_coef: 0.8295 - accuracy: 0.8312 - val_loss: 0.8034 - val_jaccard_coef: 0.1090 - val_dice_coef: 0.1966 - val_accuracy: 0.1954 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1683 - jaccard_coef: 0.7133 - dice_coef: 0.8317 - accuracy: 0.8334 - val_loss: 0.7999 - val_jaccard_coef: 0.1112 - val_dice_coef: 0.2001 - val_accuracy: 0.2008 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1689 - jaccard_coef: 0.7118 - dice_coef: 0.8311 - accuracy: 0.8329 - val_loss: 0.7362 - val_jaccard_coef: 0.1519 - val_dice_coef: 0.2638 - val_accuracy: 0.2738 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1675 - jaccard_coef: 0.7140 - dice_coef: 0.8325 - accuracy: 0.8341 - val_loss: 0.7335 - val_jaccard_coef: 0.1538 - val_dice_coef: 0.2665 - val_accuracy: 0.2778 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1682 - jaccard_coef: 0.7131 - dice_coef: 0.8318 - accuracy: 0.8334 - val_loss: 0.8264 - val_jaccard_coef: 0.0950 - val_dice_coef: 0.1736 - val_accuracy: 0.1741 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1669 - jaccard_coef: 0.7147 - dice_coef: 0.8331 - accuracy: 0.8346 - val_loss: 0.7810 - val_jaccard_coef: 0.1230 - val_dice_coef: 0.2190 - val_accuracy: 0.2170 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1673 - jaccard_coef: 0.7147 - dice_coef: 0.8327 - accuracy: 0.8342 - val_loss: 0.7591 - val_jaccard_coef: 0.1370 - val_dice_coef: 0.2409 - val_accuracy: 0.2460 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1662 - jaccard_coef: 0.7161 - dice_coef: 0.8338 - accuracy: 0.8352 - val_loss: 0.7273 - val_jaccard_coef: 0.1580 - val_dice_coef: 0.2727 - val_accuracy: 0.2796 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1643 - jaccard_coef: 0.7189 - dice_coef: 0.8357 - accuracy: 0.8370 - val_loss: 0.7349 - val_jaccard_coef: 0.1529 - val_dice_coef: 0.2651 - val_accuracy: 0.2690 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1631 - jaccard_coef: 0.7204 - dice_coef: 0.8369 - accuracy: 0.8382 - val_loss: 0.6951 - val_jaccard_coef: 0.1799 - val_dice_coef: 0.3049 - val_accuracy: 0.3100 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1633 - jaccard_coef: 0.7200 - dice_coef: 0.8367 - accuracy: 0.8381 - val_loss: 0.6463 - val_jaccard_coef: 0.2150 - val_dice_coef: 0.3537 - val_accuracy: 0.3621 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1665 - jaccard_coef: 0.7151 - dice_coef: 0.8335 - accuracy: 0.8349 - val_loss: 0.7409 - val_jaccard_coef: 0.1489 - val_dice_coef: 0.2591 - val_accuracy: 0.2693 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1630 - jaccard_coef: 0.7207 - dice_coef: 0.8370 - accuracy: 0.8382 - val_loss: 0.7021 - val_jaccard_coef: 0.1751 - val_dice_coef: 0.2979 - val_accuracy: 0.3059 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1626 - jaccard_coef: 0.7210 - dice_coef: 0.8374 - accuracy: 0.8387 - val_loss: 0.6922 - val_jaccard_coef: 0.1819 - val_dice_coef: 0.3078 - val_accuracy: 0.3140 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1601 - jaccard_coef: 0.7249 - dice_coef: 0.8399 - accuracy: 0.8410 - val_loss: 0.5984 - val_jaccard_coef: 0.2512 - val_dice_coef: 0.4016 - val_accuracy: 0.4048 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1606 - jaccard_coef: 0.7246 - dice_coef: 0.8394 - accuracy: 0.8405 - val_loss: 0.6478 - val_jaccard_coef: 0.2138 - val_dice_coef: 0.3522 - val_accuracy: 0.3554 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1706 - jaccard_coef: 0.7098 - dice_coef: 0.8294 - accuracy: 0.8308 - val_loss: 0.6508 - val_jaccard_coef: 0.2116 - val_dice_coef: 0.3492 - val_accuracy: 0.3542 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1720 - jaccard_coef: 0.7078 - dice_coef: 0.8280 - accuracy: 0.8295 - val_loss: 0.5117 - val_jaccard_coef: 0.3231 - val_dice_coef: 0.4883 - val_accuracy: 0.4965 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1763 - jaccard_coef: 0.7018 - dice_coef: 0.8237 - accuracy: 0.8250 - val_loss: 0.3174 - val_jaccard_coef: 0.5186 - val_dice_coef: 0.6826 - val_accuracy: 0.6846 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1709 - jaccard_coef: 0.7093 - dice_coef: 0.8291 - accuracy: 0.8303 - val_loss: 0.5787 - val_jaccard_coef: 0.2669 - val_dice_coef: 0.4213 - val_accuracy: 0.4218 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1657 - jaccard_coef: 0.7171 - dice_coef: 0.8343 - accuracy: 0.8355 - val_loss: 0.5885 - val_jaccard_coef: 0.2591 - val_dice_coef: 0.4115 - val_accuracy: 0.4123 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1620 - jaccard_coef: 0.7230 - dice_coef: 0.8380 - accuracy: 0.8391 - val_loss: 0.5019 - val_jaccard_coef: 0.3319 - val_dice_coef: 0.4981 - val_accuracy: 0.5006 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1593 - jaccard_coef: 0.7259 - dice_coef: 0.8407 - accuracy: 0.8416 - val_loss: 0.3806 - val_jaccard_coef: 0.4490 - val_dice_coef: 0.6194 - val_accuracy: 0.6221 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1591 - jaccard_coef: 0.7269 - dice_coef: 0.8409 - accuracy: 0.8418 - val_loss: 0.2670 - val_jaccard_coef: 0.5788 - val_dice_coef: 0.7330 - val_accuracy: 0.7351 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1586 - jaccard_coef: 0.7266 - dice_coef: 0.8414 - accuracy: 0.8422 - val_loss: 0.2248 - val_jaccard_coef: 0.6331 - val_dice_coef: 0.7752 - val_accuracy: 0.7768 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1572 - jaccard_coef: 0.7290 - dice_coef: 0.8428 - accuracy: 0.8437 - val_loss: 0.2538 - val_jaccard_coef: 0.5957 - val_dice_coef: 0.7462 - val_accuracy: 0.7479 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1568 - jaccard_coef: 0.7302 - dice_coef: 0.8432 - accuracy: 0.8441 - val_loss: 0.2170 - val_jaccard_coef: 0.6436 - val_dice_coef: 0.7830 - val_accuracy: 0.7848 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1564 - jaccard_coef: 0.7307 - dice_coef: 0.8436 - accuracy: 0.8445 - val_loss: 0.2079 - val_jaccard_coef: 0.6560 - val_dice_coef: 0.7921 - val_accuracy: 0.7933 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1564 - jaccard_coef: 0.7311 - dice_coef: 0.8436 - accuracy: 0.8445 - val_loss: 0.2135 - val_jaccard_coef: 0.6484 - val_dice_coef: 0.7865 - val_accuracy: 0.7877 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1559 - jaccard_coef: 0.7312 - dice_coef: 0.8441 - accuracy: 0.8449 - val_loss: 0.2210 - val_jaccard_coef: 0.6383 - val_dice_coef: 0.7790 - val_accuracy: 0.7804 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1559 - jaccard_coef: 0.7310 - dice_coef: 0.8441 - accuracy: 0.8450 - val_loss: 0.2015 - val_jaccard_coef: 0.6649 - val_dice_coef: 0.7985 - val_accuracy: 0.7994 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1562 - jaccard_coef: 0.7305 - dice_coef: 0.8438 - accuracy: 0.8447 - val_loss: 0.2011 - val_jaccard_coef: 0.6653 - val_dice_coef: 0.7989 - val_accuracy: 0.7999 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1571 - jaccard_coef: 0.7296 - dice_coef: 0.8429 - accuracy: 0.8438 - val_loss: 0.2067 - val_jaccard_coef: 0.6575 - val_dice_coef: 0.7933 - val_accuracy: 0.7942 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1557 - jaccard_coef: 0.7311 - dice_coef: 0.8443 - accuracy: 0.8451 - val_loss: 0.1956 - val_jaccard_coef: 0.6729 - val_dice_coef: 0.8044 - val_accuracy: 0.8053 - lr: 0.0010\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1555 - jaccard_coef: 0.7315 - dice_coef: 0.8445 - accuracy: 0.8453 - val_loss: 0.1941 - val_jaccard_coef: 0.6750 - val_dice_coef: 0.8059 - val_accuracy: 0.8067 - lr: 0.0010\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1557 - jaccard_coef: 0.7314 - dice_coef: 0.8443 - accuracy: 0.8451 - val_loss: 0.1947 - val_jaccard_coef: 0.6741 - val_dice_coef: 0.8053 - val_accuracy: 0.8060 - lr: 0.0010\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1563 - jaccard_coef: 0.7313 - dice_coef: 0.8437 - accuracy: 0.8445 - val_loss: 0.1936 - val_jaccard_coef: 0.6757 - val_dice_coef: 0.8064 - val_accuracy: 0.8070 - lr: 0.0010\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1546 - jaccard_coef: 0.7338 - dice_coef: 0.8454 - accuracy: 0.8461 - val_loss: 0.1942 - val_jaccard_coef: 0.6748 - val_dice_coef: 0.8058 - val_accuracy: 0.8064 - lr: 0.0010\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1545 - jaccard_coef: 0.7332 - dice_coef: 0.8455 - accuracy: 0.8462 - val_loss: 0.1948 - val_jaccard_coef: 0.6740 - val_dice_coef: 0.8052 - val_accuracy: 0.8058 - lr: 0.0010\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1537 - jaccard_coef: 0.7346 - dice_coef: 0.8463 - accuracy: 0.8470 - val_loss: 0.1917 - val_jaccard_coef: 0.6784 - val_dice_coef: 0.8083 - val_accuracy: 0.8089 - lr: 0.0010\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1532 - jaccard_coef: 0.7356 - dice_coef: 0.8468 - accuracy: 0.8474 - val_loss: 0.1925 - val_jaccard_coef: 0.6772 - val_dice_coef: 0.8075 - val_accuracy: 0.8080 - lr: 0.0010\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1531 - jaccard_coef: 0.7355 - dice_coef: 0.8469 - accuracy: 0.8477 - val_loss: 0.1925 - val_jaccard_coef: 0.6772 - val_dice_coef: 0.8075 - val_accuracy: 0.8082 - lr: 0.0010\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1539 - jaccard_coef: 0.7342 - dice_coef: 0.8461 - accuracy: 0.8468 - val_loss: 0.1949 - val_jaccard_coef: 0.6739 - val_dice_coef: 0.8051 - val_accuracy: 0.8057 - lr: 0.0010\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1532 - jaccard_coef: 0.7359 - dice_coef: 0.8468 - accuracy: 0.8475 - val_loss: 0.1907 - val_jaccard_coef: 0.6797 - val_dice_coef: 0.8093 - val_accuracy: 0.8098 - lr: 0.0010\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1529 - jaccard_coef: 0.7358 - dice_coef: 0.8471 - accuracy: 0.8478 - val_loss: 0.1925 - val_jaccard_coef: 0.6773 - val_dice_coef: 0.8075 - val_accuracy: 0.8081 - lr: 0.0010\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1526 - jaccard_coef: 0.7364 - dice_coef: 0.8474 - accuracy: 0.8480 - val_loss: 0.1872 - val_jaccard_coef: 0.6846 - val_dice_coef: 0.8128 - val_accuracy: 0.8133 - lr: 0.0010\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1523 - jaccard_coef: 0.7366 - dice_coef: 0.8477 - accuracy: 0.8483 - val_loss: 0.1914 - val_jaccard_coef: 0.6787 - val_dice_coef: 0.8086 - val_accuracy: 0.8091 - lr: 0.0010\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1535 - jaccard_coef: 0.7350 - dice_coef: 0.8465 - accuracy: 0.8472 - val_loss: 0.1913 - val_jaccard_coef: 0.6790 - val_dice_coef: 0.8087 - val_accuracy: 0.8092 - lr: 0.0010\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1527 - jaccard_coef: 0.7360 - dice_coef: 0.8473 - accuracy: 0.8479 - val_loss: 0.1922 - val_jaccard_coef: 0.6776 - val_dice_coef: 0.8078 - val_accuracy: 0.8084 - lr: 0.0010\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1525 - jaccard_coef: 0.7362 - dice_coef: 0.8475 - accuracy: 0.8481 - val_loss: 0.1917 - val_jaccard_coef: 0.6783 - val_dice_coef: 0.8083 - val_accuracy: 0.8085 - lr: 0.0010\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1519 - jaccard_coef: 0.7370 - dice_coef: 0.8481 - accuracy: 0.8487 - val_loss: 0.1902 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8098 - val_accuracy: 0.8105 - lr: 0.0010\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1513 - jaccard_coef: 0.7381 - dice_coef: 0.8487 - accuracy: 0.8493 - val_loss: 0.1888 - val_jaccard_coef: 0.6824 - val_dice_coef: 0.8112 - val_accuracy: 0.8117 - lr: 0.0010\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1514 - jaccard_coef: 0.7384 - dice_coef: 0.8486 - accuracy: 0.8493 - val_loss: 0.1908 - val_jaccard_coef: 0.6796 - val_dice_coef: 0.8092 - val_accuracy: 0.8098 - lr: 0.0010\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1523 - jaccard_coef: 0.7370 - dice_coef: 0.8477 - accuracy: 0.8483 - val_loss: 0.1914 - val_jaccard_coef: 0.6788 - val_dice_coef: 0.8086 - val_accuracy: 0.8093 - lr: 0.0010\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1514 - jaccard_coef: 0.7384 - dice_coef: 0.8486 - accuracy: 0.8492 - val_loss: 0.1931 - val_jaccard_coef: 0.6764 - val_dice_coef: 0.8069 - val_accuracy: 0.8074 - lr: 0.0010\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1505 - jaccard_coef: 0.7396 - dice_coef: 0.8495 - accuracy: 0.8501 - val_loss: 0.1926 - val_jaccard_coef: 0.6771 - val_dice_coef: 0.8074 - val_accuracy: 0.8079 - lr: 0.0010\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1505 - jaccard_coef: 0.7391 - dice_coef: 0.8495 - accuracy: 0.8501 - val_loss: 0.1930 - val_jaccard_coef: 0.6764 - val_dice_coef: 0.8070 - val_accuracy: 0.8072 - lr: 0.0010\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1505 - jaccard_coef: 0.7392 - dice_coef: 0.8495 - accuracy: 0.8501 - val_loss: 0.1890 - val_jaccard_coef: 0.6821 - val_dice_coef: 0.8110 - val_accuracy: 0.8115 - lr: 0.0010\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1508 - jaccard_coef: 0.7390 - dice_coef: 0.8492 - accuracy: 0.8498 - val_loss: 0.1922 - val_jaccard_coef: 0.6777 - val_dice_coef: 0.8078 - val_accuracy: 0.8084 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1502 - jaccard_coef: 0.7402 - dice_coef: 0.8498 - accuracy: 0.8503 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8105 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1505 - jaccard_coef: 0.7393 - dice_coef: 0.8495 - accuracy: 0.8500 - val_loss: 0.1916 - val_jaccard_coef: 0.6784 - val_dice_coef: 0.8084 - val_accuracy: 0.8089 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1501 - jaccard_coef: 0.7398 - dice_coef: 0.8499 - accuracy: 0.8504 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8103 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1500 - jaccard_coef: 0.7409 - dice_coef: 0.8500 - accuracy: 0.8505 - val_loss: 0.1909 - val_jaccard_coef: 0.6794 - val_dice_coef: 0.8091 - val_accuracy: 0.8095 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1504 - jaccard_coef: 0.7393 - dice_coef: 0.8496 - accuracy: 0.8502 - val_loss: 0.1925 - val_jaccard_coef: 0.6771 - val_dice_coef: 0.8075 - val_accuracy: 0.8081 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1503 - jaccard_coef: 0.7396 - dice_coef: 0.8497 - accuracy: 0.8503 - val_loss: 0.1925 - val_jaccard_coef: 0.6773 - val_dice_coef: 0.8075 - val_accuracy: 0.8080 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1509 - jaccard_coef: 0.7395 - dice_coef: 0.8491 - accuracy: 0.8496\n",
            "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1509 - jaccard_coef: 0.7395 - dice_coef: 0.8491 - accuracy: 0.8496 - val_loss: 0.1935 - val_jaccard_coef: 0.6758 - val_dice_coef: 0.8065 - val_accuracy: 0.8069 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1494 - jaccard_coef: 0.7415 - dice_coef: 0.8506 - accuracy: 0.8511 - val_loss: 0.1896 - val_jaccard_coef: 0.6813 - val_dice_coef: 0.8104 - val_accuracy: 0.8109 - lr: 3.0000e-04\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1483 - jaccard_coef: 0.7434 - dice_coef: 0.8517 - accuracy: 0.8522 - val_loss: 0.1898 - val_jaccard_coef: 0.6811 - val_dice_coef: 0.8102 - val_accuracy: 0.8105 - lr: 3.0000e-04\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1478 - jaccard_coef: 0.7437 - dice_coef: 0.8522 - accuracy: 0.8528 - val_loss: 0.1890 - val_jaccard_coef: 0.6821 - val_dice_coef: 0.8110 - val_accuracy: 0.8113 - lr: 3.0000e-04\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1474 - jaccard_coef: 0.7436 - dice_coef: 0.8526 - accuracy: 0.8531 - val_loss: 0.1910 - val_jaccard_coef: 0.6793 - val_dice_coef: 0.8090 - val_accuracy: 0.8093 - lr: 3.0000e-04\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1476 - jaccard_coef: 0.7437 - dice_coef: 0.8524 - accuracy: 0.8530 - val_loss: 0.1907 - val_jaccard_coef: 0.6797 - val_dice_coef: 0.8093 - val_accuracy: 0.8096 - lr: 3.0000e-04\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1473 - jaccard_coef: 0.7441 - dice_coef: 0.8527 - accuracy: 0.8532 - val_loss: 0.1906 - val_jaccard_coef: 0.6799 - val_dice_coef: 0.8094 - val_accuracy: 0.8096 - lr: 3.0000e-04\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1470 - jaccard_coef: 0.7445 - dice_coef: 0.8530 - accuracy: 0.8535 - val_loss: 0.1901 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8099 - val_accuracy: 0.8102 - lr: 3.0000e-04\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1472 - jaccard_coef: 0.7443 - dice_coef: 0.8528 - accuracy: 0.8533 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 3.0000e-04\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1469 - jaccard_coef: 0.7450 - dice_coef: 0.8531 - accuracy: 0.8536 - val_loss: 0.1887 - val_jaccard_coef: 0.6826 - val_dice_coef: 0.8113 - val_accuracy: 0.8117 - lr: 3.0000e-04\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1467 - jaccard_coef: 0.7453 - dice_coef: 0.8533 - accuracy: 0.8538 - val_loss: 0.1886 - val_jaccard_coef: 0.6827 - val_dice_coef: 0.8114 - val_accuracy: 0.8117 - lr: 3.0000e-04\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1466 - jaccard_coef: 0.7455 - dice_coef: 0.8534 - accuracy: 0.8539 - val_loss: 0.1910 - val_jaccard_coef: 0.6793 - val_dice_coef: 0.8090 - val_accuracy: 0.8094 - lr: 3.0000e-04\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1465 - jaccard_coef: 0.7457 - dice_coef: 0.8535 - accuracy: 0.8540 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 3.0000e-04\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1465 - jaccard_coef: 0.7455 - dice_coef: 0.8535 - accuracy: 0.8540 - val_loss: 0.1905 - val_jaccard_coef: 0.6801 - val_dice_coef: 0.8095 - val_accuracy: 0.8098 - lr: 3.0000e-04\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1464 - jaccard_coef: 0.7458 - dice_coef: 0.8536 - accuracy: 0.8541 - val_loss: 0.1906 - val_jaccard_coef: 0.6798 - val_dice_coef: 0.8094 - val_accuracy: 0.8097 - lr: 3.0000e-04\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1464 - jaccard_coef: 0.7453 - dice_coef: 0.8536 - accuracy: 0.8541 - val_loss: 0.1909 - val_jaccard_coef: 0.6794 - val_dice_coef: 0.8091 - val_accuracy: 0.8094 - lr: 3.0000e-04\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1467 - jaccard_coef: 0.7458 - dice_coef: 0.8533 - accuracy: 0.8538 - val_loss: 0.1907 - val_jaccard_coef: 0.6797 - val_dice_coef: 0.8093 - val_accuracy: 0.8096 - lr: 3.0000e-04\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1467 - jaccard_coef: 0.7453 - dice_coef: 0.8533 - accuracy: 0.8538 - val_loss: 0.1904 - val_jaccard_coef: 0.6802 - val_dice_coef: 0.8096 - val_accuracy: 0.8099 - lr: 3.0000e-04\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1463 - jaccard_coef: 0.7458 - dice_coef: 0.8537 - accuracy: 0.8542 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8102 - lr: 3.0000e-04\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1463 - jaccard_coef: 0.7463 - dice_coef: 0.8537 - accuracy: 0.8541 - val_loss: 0.1903 - val_jaccard_coef: 0.6803 - val_dice_coef: 0.8097 - val_accuracy: 0.8101 - lr: 3.0000e-04\n",
            "Epoch 119/200\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.1430 - jaccard_coef: 0.7506 - dice_coef: 0.8570 - accuracy: 0.8574\n",
            "Epoch 00119: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1462 - jaccard_coef: 0.7460 - dice_coef: 0.8538 - accuracy: 0.8543 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8104 - lr: 3.0000e-04\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1459 - jaccard_coef: 0.7462 - dice_coef: 0.8541 - accuracy: 0.8546 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8103 - lr: 9.0000e-05\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1458 - jaccard_coef: 0.7469 - dice_coef: 0.8542 - accuracy: 0.8547 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 9.0000e-05\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 1s 47ms/step - loss: 0.1457 - jaccard_coef: 0.7471 - dice_coef: 0.8543 - accuracy: 0.8548 - val_loss: 0.1897 - val_jaccard_coef: 0.6812 - val_dice_coef: 0.8103 - val_accuracy: 0.8107 - lr: 9.0000e-05\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1457 - jaccard_coef: 0.7468 - dice_coef: 0.8543 - accuracy: 0.8548 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8104 - lr: 9.0000e-05\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1457 - jaccard_coef: 0.7466 - dice_coef: 0.8543 - accuracy: 0.8548 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 9.0000e-05\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1457 - jaccard_coef: 0.7467 - dice_coef: 0.8543 - accuracy: 0.8547 - val_loss: 0.1903 - val_jaccard_coef: 0.6803 - val_dice_coef: 0.8097 - val_accuracy: 0.8100 - lr: 9.0000e-05\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1454 - jaccard_coef: 0.7465 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1901 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8099 - val_accuracy: 0.8101 - lr: 9.0000e-05\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1455 - jaccard_coef: 0.7472 - dice_coef: 0.8545 - accuracy: 0.8549 - val_loss: 0.1896 - val_jaccard_coef: 0.6813 - val_dice_coef: 0.8104 - val_accuracy: 0.8108 - lr: 9.0000e-05\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1455 - jaccard_coef: 0.7468 - dice_coef: 0.8545 - accuracy: 0.8550 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8102 - lr: 9.0000e-05\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1456 - jaccard_coef: 0.7468 - dice_coef: 0.8544 - accuracy: 0.8549 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8100 - lr: 9.0000e-05\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1454 - jaccard_coef: 0.7475 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1905 - val_jaccard_coef: 0.6801 - val_dice_coef: 0.8095 - val_accuracy: 0.8098 - lr: 9.0000e-05\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1455 - jaccard_coef: 0.7470 - dice_coef: 0.8545 - accuracy: 0.8550 - val_loss: 0.1901 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8099 - val_accuracy: 0.8101 - lr: 9.0000e-05\n",
            "Epoch 132/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1454 - jaccard_coef: 0.7467 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 9.0000e-05\n",
            "Epoch 133/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1454 - jaccard_coef: 0.7470 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1893 - val_jaccard_coef: 0.6817 - val_dice_coef: 0.8107 - val_accuracy: 0.8110 - lr: 9.0000e-05\n",
            "Epoch 134/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1454 - jaccard_coef: 0.7469 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1894 - val_jaccard_coef: 0.6815 - val_dice_coef: 0.8106 - val_accuracy: 0.8108 - lr: 9.0000e-05\n",
            "Epoch 135/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1454 - jaccard_coef: 0.7473 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 9.0000e-05\n",
            "Epoch 136/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1457 - jaccard_coef: 0.7470 - dice_coef: 0.8543 - accuracy: 0.8549 - val_loss: 0.1897 - val_jaccard_coef: 0.6811 - val_dice_coef: 0.8103 - val_accuracy: 0.8107 - lr: 9.0000e-05\n",
            "Epoch 137/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1454 - jaccard_coef: 0.7471 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 9.0000e-05\n",
            "Epoch 138/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1453 - jaccard_coef: 0.7473 - dice_coef: 0.8547 - accuracy: 0.8552 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8101 - lr: 9.0000e-05\n",
            "Epoch 139/200\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.1452 - jaccard_coef: 0.7476 - dice_coef: 0.8548 - accuracy: 0.8552\n",
            "Epoch 00139: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1454 - jaccard_coef: 0.7473 - dice_coef: 0.8546 - accuracy: 0.8551 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8103 - lr: 9.0000e-05\n",
            "Epoch 140/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1452 - jaccard_coef: 0.7475 - dice_coef: 0.8548 - accuracy: 0.8553 - val_loss: 0.1902 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 2.7000e-05\n",
            "Epoch 141/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1452 - jaccard_coef: 0.7475 - dice_coef: 0.8548 - accuracy: 0.8552 - val_loss: 0.1902 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 2.7000e-05\n",
            "Epoch 142/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7477 - dice_coef: 0.8549 - accuracy: 0.8553 - val_loss: 0.1901 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8099 - val_accuracy: 0.8102 - lr: 2.7000e-05\n",
            "Epoch 143/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7478 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1902 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 2.7000e-05\n",
            "Epoch 144/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1453 - jaccard_coef: 0.7484 - dice_coef: 0.8547 - accuracy: 0.8552 - val_loss: 0.1900 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 145/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1452 - jaccard_coef: 0.7479 - dice_coef: 0.8548 - accuracy: 0.8553 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 2.7000e-05\n",
            "Epoch 146/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1453 - jaccard_coef: 0.7474 - dice_coef: 0.8547 - accuracy: 0.8552 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 147/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7483 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 2.7000e-05\n",
            "Epoch 148/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1450 - jaccard_coef: 0.7475 - dice_coef: 0.8550 - accuracy: 0.8554 - val_loss: 0.1900 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 149/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1450 - jaccard_coef: 0.7471 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 150/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7474 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 2.7000e-05\n",
            "Epoch 151/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7473 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1898 - val_jaccard_coef: 0.6810 - val_dice_coef: 0.8102 - val_accuracy: 0.8105 - lr: 2.7000e-05\n",
            "Epoch 152/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1449 - jaccard_coef: 0.7478 - dice_coef: 0.8551 - accuracy: 0.8555 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 153/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7479 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 154/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1451 - jaccard_coef: 0.7471 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1898 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8102 - val_accuracy: 0.8104 - lr: 2.7000e-05\n",
            "Epoch 155/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1452 - jaccard_coef: 0.7475 - dice_coef: 0.8548 - accuracy: 0.8553 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 156/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7477 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 157/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1452 - jaccard_coef: 0.7473 - dice_coef: 0.8548 - accuracy: 0.8553 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 2.7000e-05\n",
            "Epoch 158/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1453 - jaccard_coef: 0.7475 - dice_coef: 0.8547 - accuracy: 0.8552 - val_loss: 0.1898 - val_jaccard_coef: 0.6810 - val_dice_coef: 0.8102 - val_accuracy: 0.8106 - lr: 2.7000e-05\n",
            "Epoch 159/200\n",
            "17/18 [===========================>..] - ETA: 0s - loss: 0.1451 - jaccard_coef: 0.7477 - dice_coef: 0.8549 - accuracy: 0.8553\n",
            "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1449 - jaccard_coef: 0.7481 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 2.7000e-05\n",
            "Epoch 160/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1451 - jaccard_coef: 0.7474 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1451 - jaccard_coef: 0.7479 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1449 - jaccard_coef: 0.7481 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1898 - val_jaccard_coef: 0.6810 - val_dice_coef: 0.8102 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7481 - dice_coef: 0.8551 - accuracy: 0.8555 - val_loss: 0.1898 - val_jaccard_coef: 0.6810 - val_dice_coef: 0.8102 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1449 - jaccard_coef: 0.7476 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1898 - val_jaccard_coef: 0.6810 - val_dice_coef: 0.8102 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1450 - jaccard_coef: 0.7475 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1449 - jaccard_coef: 0.7479 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1450 - jaccard_coef: 0.7475 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1448 - jaccard_coef: 0.7484 - dice_coef: 0.8552 - accuracy: 0.8557 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1449 - jaccard_coef: 0.7477 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1902 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7485 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1450 - jaccard_coef: 0.7476 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1902 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8098 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1450 - jaccard_coef: 0.7485 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1901 - val_jaccard_coef: 0.6805 - val_dice_coef: 0.8099 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1448 - jaccard_coef: 0.7486 - dice_coef: 0.8552 - accuracy: 0.8556 - val_loss: 0.1902 - val_jaccard_coef: 0.6804 - val_dice_coef: 0.8098 - val_accuracy: 0.8101 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1450 - jaccard_coef: 0.7483 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1901 - val_jaccard_coef: 0.6806 - val_dice_coef: 0.8099 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1451 - jaccard_coef: 0.7479 - dice_coef: 0.8549 - accuracy: 0.8554 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1450 - jaccard_coef: 0.7476 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1452 - jaccard_coef: 0.7474 - dice_coef: 0.8548 - accuracy: 0.8554 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1450 - jaccard_coef: 0.7482 - dice_coef: 0.8550 - accuracy: 0.8554 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1448 - jaccard_coef: 0.7478 - dice_coef: 0.8552 - accuracy: 0.8557 - val_loss: 0.1900 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7485 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1450 - jaccard_coef: 0.7478 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1448 - jaccard_coef: 0.7481 - dice_coef: 0.8552 - accuracy: 0.8557 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1449 - jaccard_coef: 0.7481 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1450 - jaccard_coef: 0.7476 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1453 - jaccard_coef: 0.7478 - dice_coef: 0.8547 - accuracy: 0.8552 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1449 - jaccard_coef: 0.7477 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1450 - jaccard_coef: 0.7476 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1449 - jaccard_coef: 0.7482 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.1449 - jaccard_coef: 0.7478 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.1449 - jaccard_coef: 0.7480 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1452 - jaccard_coef: 0.7480 - dice_coef: 0.8548 - accuracy: 0.8553 - val_loss: 0.1900 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1449 - jaccard_coef: 0.7483 - dice_coef: 0.8551 - accuracy: 0.8557 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1448 - jaccard_coef: 0.7487 - dice_coef: 0.8552 - accuracy: 0.8557 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1450 - jaccard_coef: 0.7481 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1450 - jaccard_coef: 0.7476 - dice_coef: 0.8550 - accuracy: 0.8554 - val_loss: 0.1899 - val_jaccard_coef: 0.6809 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7477 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "18/18 [==============================] - 1s 51ms/step - loss: 0.1450 - jaccard_coef: 0.7481 - dice_coef: 0.8550 - accuracy: 0.8555 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8102 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7483 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.1449 - jaccard_coef: 0.7481 - dice_coef: 0.8551 - accuracy: 0.8556 - val_loss: 0.1900 - val_jaccard_coef: 0.6807 - val_dice_coef: 0.8100 - val_accuracy: 0.8103 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "18/18 [==============================] - 1s 50ms/step - loss: 0.1448 - jaccard_coef: 0.7484 - dice_coef: 0.8552 - accuracy: 0.8557 - val_loss: 0.1899 - val_jaccard_coef: 0.6808 - val_dice_coef: 0.8101 - val_accuracy: 0.8104 - lr: 1.0000e-05\n",
            "time:  177.83081531524658\n"
          ]
        }
      ],
      "source": [
        "now = time.time()\n",
        "history = model_psp.fit(X1, y_train_cat, \n",
        "                    batch_size = 8, \n",
        "                    verbose=1, \n",
        "                    epochs=200, \n",
        "                    validation_data=(X_test, y_test_cat),shuffle=True,\n",
        "                   callbacks = [lrd , mcp])\n",
        "end = time.time()\n",
        "print('time: ', end - now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dae8372",
      "metadata": {
        "id": "8dae8372"
      },
      "outputs": [],
      "source": [
        "#model_psp.save('psp.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3f974d94",
      "metadata": {
        "id": "3f974d94"
      },
      "outputs": [],
      "source": [
        "with open('psp.pickle' , 'wb') as file:\n",
        "  pickle.dump(history.history,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "ea978661",
      "metadata": {
        "id": "ea978661"
      },
      "outputs": [],
      "source": [
        "pred_train = model_psp.predict(X1)\n",
        "pred_val = model_psp.predict(X_test)\n",
        "pred_test = model_psp.predict(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "Dp4OVkpmnmdh",
      "metadata": {
        "id": "Dp4OVkpmnmdh"
      },
      "outputs": [],
      "source": [
        "pred_train1 = np.argmax(pred_train, axis = 3)\n",
        "pred_val1 = np.argmax(pred_val, axis = 3)\n",
        "pred_test1 = np.argmax(pred_test, axis = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "vtsWvAmF9vwn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtsWvAmF9vwn",
        "outputId": "15280854-234a-49dc-ec7d-e8dbe6340e93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4426431"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(y_train_cat, pred_train)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "B4lnONKZnvbL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4lnONKZnvbL",
        "outputId": "75db42fe-cf3e-4c82-fe63-f6a92036a093"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48635918"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state( y_test_cat, pred_val)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ROh70I20nmhq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROh70I20nmhq",
        "outputId": "ebebe573-e557-4a94-a02f-b2fcbac4d8ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48179647"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(test_masks_cat, pred_test)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5231b64a",
      "metadata": {
        "id": "5231b64a"
      },
      "outputs": [],
      "source": [
        "#========================================================================================================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e588fe44",
      "metadata": {
        "id": "e588fe44"
      },
      "outputs": [],
      "source": [
        "#DeepLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b68ed218",
      "metadata": {
        "id": "b68ed218"
      },
      "outputs": [],
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1d6d2c5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d6d2c5b",
        "outputId": "1e8d69a2-4935-4016-bbee-2d6cdbf3f46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 134, 134, 3)  0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 64, 64, 64)   9472        ['conv1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 64, 64, 64)   256         ['conv1_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 64, 64, 64)   0           ['conv1_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 66, 66, 64)   0           ['conv1_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 32, 32, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 32, 32, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 32, 32, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 32, 32, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 32, 32, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 32, 32, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 32, 32, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 32, 32, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 32, 32, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 32, 32, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 32, 32, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 32, 32, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 32, 32, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 32, 32, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 16, 16, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 16, 16, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 16, 16, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 16, 16, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 16, 16, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 16, 16, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 16, 16, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 16, 16, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 16, 16, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 16, 16, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 16, 16, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 16, 16, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 16, 16, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 16, 16, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 16, 16, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 8, 8, 256)    131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 8, 8, 1024)   525312      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                                                  'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block1_out[0][0]',       \n",
            "                                                                  'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block2_out[0][0]',       \n",
            "                                                                  'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block3_out[0][0]',       \n",
            "                                                                  'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 8, 8, 1024)   263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 8, 8, 1024)  4096        ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 8, 8, 1024)   0           ['conv4_block4_out[0][0]',       \n",
            "                                                                  'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 8, 8, 1024)   0           ['conv4_block5_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 8, 8, 256)    262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 8, 8, 256)    590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 8, 8, 256)   1024        ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 8, 8, 256)   0           ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 1, 1, 256)   0           ['conv4_block6_2_relu[0][0]']    \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 1, 1, 256)    65792       ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 1, 1, 256)   1024        ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 8, 8, 256)    65536       ['conv4_block6_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 8, 8, 256)    589824      ['conv4_block6_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 8, 8, 256)    589824      ['conv4_block6_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 8, 8, 256)    589824      ['conv4_block6_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " tf.nn.relu (TFOpLambda)        (None, 1, 1, 256)    0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_32[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 256)   0           ['tf.nn.relu[0][0]']             \n",
            "                                                                                                  \n",
            " tf.nn.relu_1 (TFOpLambda)      (None, 8, 8, 256)    0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " tf.nn.relu_2 (TFOpLambda)      (None, 8, 8, 256)    0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " tf.nn.relu_3 (TFOpLambda)      (None, 8, 8, 256)    0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " tf.nn.relu_4 (TFOpLambda)      (None, 8, 8, 256)    0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 8, 8, 1280)   0           ['up_sampling2d_4[0][0]',        \n",
            "                                                                  'tf.nn.relu_1[0][0]',           \n",
            "                                                                  'tf.nn.relu_2[0][0]',           \n",
            "                                                                  'tf.nn.relu_3[0][0]',           \n",
            "                                                                  'tf.nn.relu_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 8, 8, 256)    327680      ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_33[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 32, 32, 48)   3072        ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " tf.nn.relu_5 (TFOpLambda)      (None, 8, 8, 256)    0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 32, 32, 48)  192         ['conv2d_34[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 256)  0          ['tf.nn.relu_5[0][0]']           \n",
            "                                                                                                  \n",
            " tf.nn.relu_6 (TFOpLambda)      (None, 32, 32, 48)   0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 32, 32, 304)  0           ['up_sampling2d_5[0][0]',        \n",
            "                                                                  'tf.nn.relu_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 32, 32, 256)  700416      ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_35[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.nn.relu_7 (TFOpLambda)      (None, 32, 32, 256)  0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 32, 32, 256)  0           ['tf.nn.relu_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 32, 32, 256)  589824      ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_36[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.nn.relu_8 (TFOpLambda)      (None, 32, 32, 256)  0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 32, 32, 256)  0           ['tf.nn.relu_8[0][0]']           \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 25  0          ['dropout_5[0][0]']              \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 128, 128, 25  0           ['up_sampling2d_6[0][0]']        \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 128, 128, 31  7967        ['dropout_6[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,860,063\n",
            "Trainable params: 11,827,327\n",
            "Non-trainable params: 32,736\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = convolution_block(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    model_output = Conv2D(num_classes, kernel_size=(1, 1), activation='softmax')(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model_deeplab = DeeplabV3Plus(image_size=128, num_classes=31)\n",
        "model_deeplab.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c76e2af7",
      "metadata": {
        "id": "c76e2af7"
      },
      "outputs": [],
      "source": [
        "model_deeplab.compile(optimizer='adam', \n",
        "              loss= dice_coef_loss,\n",
        "              metrics=[jaccard_coef,dice_coef,'accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0ba8a4cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ba8a4cd",
        "outputId": "fc0934ba-72fb-479d-bfe7-654bb1b8adf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "18/18 [==============================] - 10s 166ms/step - loss: 0.4592 - jaccard_coef: 0.3865 - dice_coef: 0.5408 - accuracy: 0.5726 - val_loss: 0.7340 - val_jaccard_coef: 0.1534 - val_dice_coef: 0.2660 - val_accuracy: 0.2662 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "18/18 [==============================] - 2s 111ms/step - loss: 0.3486 - jaccard_coef: 0.4845 - dice_coef: 0.6514 - accuracy: 0.6538 - val_loss: 0.8171 - val_jaccard_coef: 0.1007 - val_dice_coef: 0.1829 - val_accuracy: 0.1825 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.3092 - jaccard_coef: 0.5291 - dice_coef: 0.6908 - accuracy: 0.6951 - val_loss: 0.8405 - val_jaccard_coef: 0.0867 - val_dice_coef: 0.1595 - val_accuracy: 0.1598 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.2816 - jaccard_coef: 0.5614 - dice_coef: 0.7184 - accuracy: 0.7220 - val_loss: 0.7112 - val_jaccard_coef: 0.1688 - val_dice_coef: 0.2888 - val_accuracy: 0.2908 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.2759 - jaccard_coef: 0.5689 - dice_coef: 0.7241 - accuracy: 0.7273 - val_loss: 0.7206 - val_jaccard_coef: 0.1624 - val_dice_coef: 0.2794 - val_accuracy: 0.2843 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.2767 - jaccard_coef: 0.5681 - dice_coef: 0.7233 - accuracy: 0.7260 - val_loss: 0.6838 - val_jaccard_coef: 0.1878 - val_dice_coef: 0.3162 - val_accuracy: 0.3161 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.2647 - jaccard_coef: 0.5829 - dice_coef: 0.7353 - accuracy: 0.7378 - val_loss: 0.7898 - val_jaccard_coef: 0.1175 - val_dice_coef: 0.2102 - val_accuracy: 0.2121 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.2543 - jaccard_coef: 0.5964 - dice_coef: 0.7457 - accuracy: 0.7482 - val_loss: 0.6081 - val_jaccard_coef: 0.2437 - val_dice_coef: 0.3919 - val_accuracy: 0.3932 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "18/18 [==============================] - 2s 111ms/step - loss: 0.2464 - jaccard_coef: 0.6053 - dice_coef: 0.7536 - accuracy: 0.7559 - val_loss: 0.6169 - val_jaccard_coef: 0.2369 - val_dice_coef: 0.3831 - val_accuracy: 0.3835 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.2456 - jaccard_coef: 0.6073 - dice_coef: 0.7544 - accuracy: 0.7566 - val_loss: 0.5039 - val_jaccard_coef: 0.3299 - val_dice_coef: 0.4961 - val_accuracy: 0.5278 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.2392 - jaccard_coef: 0.6145 - dice_coef: 0.7608 - accuracy: 0.7627 - val_loss: 0.5375 - val_jaccard_coef: 0.3009 - val_dice_coef: 0.4625 - val_accuracy: 0.4767 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.2385 - jaccard_coef: 0.6177 - dice_coef: 0.7615 - accuracy: 0.7632 - val_loss: 0.5437 - val_jaccard_coef: 0.2956 - val_dice_coef: 0.4563 - val_accuracy: 0.4757 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "18/18 [==============================] - 2s 111ms/step - loss: 0.2330 - jaccard_coef: 0.6238 - dice_coef: 0.7670 - accuracy: 0.7693 - val_loss: 0.5971 - val_jaccard_coef: 0.2522 - val_dice_coef: 0.4029 - val_accuracy: 0.4178 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.2105 - jaccard_coef: 0.6529 - dice_coef: 0.7895 - accuracy: 0.7938 - val_loss: 0.6493 - val_jaccard_coef: 0.2126 - val_dice_coef: 0.3507 - val_accuracy: 0.3462 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1973 - jaccard_coef: 0.6711 - dice_coef: 0.8027 - accuracy: 0.8059 - val_loss: 0.6246 - val_jaccard_coef: 0.2311 - val_dice_coef: 0.3754 - val_accuracy: 0.3977 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1878 - jaccard_coef: 0.6849 - dice_coef: 0.8122 - accuracy: 0.8148 - val_loss: 0.6042 - val_jaccard_coef: 0.2467 - val_dice_coef: 0.3958 - val_accuracy: 0.4359 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1857 - jaccard_coef: 0.6884 - dice_coef: 0.8143 - accuracy: 0.8168 - val_loss: 0.6031 - val_jaccard_coef: 0.2477 - val_dice_coef: 0.3969 - val_accuracy: 0.4075 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1783 - jaccard_coef: 0.6980 - dice_coef: 0.8217 - accuracy: 0.8239 - val_loss: 0.6260 - val_jaccard_coef: 0.2301 - val_dice_coef: 0.3740 - val_accuracy: 0.3835 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "18/18 [==============================] - 2s 111ms/step - loss: 0.1790 - jaccard_coef: 0.6975 - dice_coef: 0.8210 - accuracy: 0.8231 - val_loss: 0.6519 - val_jaccard_coef: 0.2107 - val_dice_coef: 0.3481 - val_accuracy: 0.3610 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1744 - jaccard_coef: 0.7040 - dice_coef: 0.8256 - accuracy: 0.8277 - val_loss: 0.6190 - val_jaccard_coef: 0.2353 - val_dice_coef: 0.3810 - val_accuracy: 0.4203 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1753 - jaccard_coef: 0.7022 - dice_coef: 0.8247 - accuracy: 0.8267 - val_loss: 0.5996 - val_jaccard_coef: 0.2503 - val_dice_coef: 0.4004 - val_accuracy: 0.4517 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1739 - jaccard_coef: 0.7045 - dice_coef: 0.8261 - accuracy: 0.8282 - val_loss: 0.6293 - val_jaccard_coef: 0.2275 - val_dice_coef: 0.3707 - val_accuracy: 0.3953 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1702 - jaccard_coef: 0.7095 - dice_coef: 0.8298 - accuracy: 0.8317 - val_loss: 0.7845 - val_jaccard_coef: 0.1208 - val_dice_coef: 0.2155 - val_accuracy: 0.1978 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1693 - jaccard_coef: 0.7115 - dice_coef: 0.8307 - accuracy: 0.8326 - val_loss: 0.6924 - val_jaccard_coef: 0.1818 - val_dice_coef: 0.3076 - val_accuracy: 0.2996 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1671 - jaccard_coef: 0.7149 - dice_coef: 0.8329 - accuracy: 0.8347 - val_loss: 0.5893 - val_jaccard_coef: 0.2584 - val_dice_coef: 0.4107 - val_accuracy: 0.4411 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1662 - jaccard_coef: 0.7158 - dice_coef: 0.8338 - accuracy: 0.8356 - val_loss: 0.5594 - val_jaccard_coef: 0.2825 - val_dice_coef: 0.4406 - val_accuracy: 0.4811 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1663 - jaccard_coef: 0.7161 - dice_coef: 0.8337 - accuracy: 0.8355 - val_loss: 0.5148 - val_jaccard_coef: 0.3203 - val_dice_coef: 0.4852 - val_accuracy: 0.5275 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1654 - jaccard_coef: 0.7172 - dice_coef: 0.8346 - accuracy: 0.8363 - val_loss: 0.4910 - val_jaccard_coef: 0.3414 - val_dice_coef: 0.5090 - val_accuracy: 0.5519 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1643 - jaccard_coef: 0.7185 - dice_coef: 0.8357 - accuracy: 0.8374 - val_loss: 0.5282 - val_jaccard_coef: 0.3087 - val_dice_coef: 0.4718 - val_accuracy: 0.5029 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1619 - jaccard_coef: 0.7221 - dice_coef: 0.8381 - accuracy: 0.8397 - val_loss: 0.4865 - val_jaccard_coef: 0.3454 - val_dice_coef: 0.5135 - val_accuracy: 0.5716 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1613 - jaccard_coef: 0.7227 - dice_coef: 0.8387 - accuracy: 0.8403 - val_loss: 0.5369 - val_jaccard_coef: 0.3014 - val_dice_coef: 0.4631 - val_accuracy: 0.4808 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "18/18 [==============================] - 2s 112ms/step - loss: 0.1605 - jaccard_coef: 0.7243 - dice_coef: 0.8395 - accuracy: 0.8411 - val_loss: 0.6520 - val_jaccard_coef: 0.2107 - val_dice_coef: 0.3480 - val_accuracy: 0.3482 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1604 - jaccard_coef: 0.7242 - dice_coef: 0.8396 - accuracy: 0.8412 - val_loss: 0.5643 - val_jaccard_coef: 0.2786 - val_dice_coef: 0.4357 - val_accuracy: 0.4502 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1595 - jaccard_coef: 0.7258 - dice_coef: 0.8405 - accuracy: 0.8419 - val_loss: 0.4986 - val_jaccard_coef: 0.3347 - val_dice_coef: 0.5014 - val_accuracy: 0.5319 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1584 - jaccard_coef: 0.7275 - dice_coef: 0.8416 - accuracy: 0.8431 - val_loss: 0.4414 - val_jaccard_coef: 0.3876 - val_dice_coef: 0.5586 - val_accuracy: 0.6004 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1585 - jaccard_coef: 0.7283 - dice_coef: 0.8415 - accuracy: 0.8429 - val_loss: 0.4172 - val_jaccard_coef: 0.4112 - val_dice_coef: 0.5828 - val_accuracy: 0.6081 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1576 - jaccard_coef: 0.7287 - dice_coef: 0.8424 - accuracy: 0.8439 - val_loss: 0.4111 - val_jaccard_coef: 0.4174 - val_dice_coef: 0.5889 - val_accuracy: 0.5996 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1566 - jaccard_coef: 0.7303 - dice_coef: 0.8434 - accuracy: 0.8448 - val_loss: 0.3960 - val_jaccard_coef: 0.4327 - val_dice_coef: 0.6040 - val_accuracy: 0.6158 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1563 - jaccard_coef: 0.7305 - dice_coef: 0.8437 - accuracy: 0.8451 - val_loss: 0.3904 - val_jaccard_coef: 0.4386 - val_dice_coef: 0.6096 - val_accuracy: 0.6173 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1559 - jaccard_coef: 0.7321 - dice_coef: 0.8441 - accuracy: 0.8454 - val_loss: 0.3911 - val_jaccard_coef: 0.4378 - val_dice_coef: 0.6089 - val_accuracy: 0.6159 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1559 - jaccard_coef: 0.7316 - dice_coef: 0.8441 - accuracy: 0.8455 - val_loss: 0.4090 - val_jaccard_coef: 0.4196 - val_dice_coef: 0.5910 - val_accuracy: 0.5986 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1551 - jaccard_coef: 0.7325 - dice_coef: 0.8449 - accuracy: 0.8462 - val_loss: 0.3742 - val_jaccard_coef: 0.4556 - val_dice_coef: 0.6258 - val_accuracy: 0.6293 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1529 - jaccard_coef: 0.7357 - dice_coef: 0.8471 - accuracy: 0.8487 - val_loss: 0.4131 - val_jaccard_coef: 0.4154 - val_dice_coef: 0.5869 - val_accuracy: 0.5975 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1501 - jaccard_coef: 0.7401 - dice_coef: 0.8499 - accuracy: 0.8524 - val_loss: 0.3847 - val_jaccard_coef: 0.4444 - val_dice_coef: 0.6153 - val_accuracy: 0.6204 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1528 - jaccard_coef: 0.7360 - dice_coef: 0.8472 - accuracy: 0.8494 - val_loss: 0.6148 - val_jaccard_coef: 0.2391 - val_dice_coef: 0.3852 - val_accuracy: 0.3879 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1491 - jaccard_coef: 0.7413 - dice_coef: 0.8509 - accuracy: 0.8526 - val_loss: 0.5677 - val_jaccard_coef: 0.2758 - val_dice_coef: 0.4323 - val_accuracy: 0.4337 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1497 - jaccard_coef: 0.7409 - dice_coef: 0.8503 - accuracy: 0.8523 - val_loss: 0.5554 - val_jaccard_coef: 0.2859 - val_dice_coef: 0.4446 - val_accuracy: 0.4463 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1515 - jaccard_coef: 0.7378 - dice_coef: 0.8485 - accuracy: 0.8504 - val_loss: 0.4945 - val_jaccard_coef: 0.3387 - val_dice_coef: 0.5055 - val_accuracy: 0.5081 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1688 - jaccard_coef: 0.7124 - dice_coef: 0.8312 - accuracy: 0.8332 - val_loss: 0.5964 - val_jaccard_coef: 0.2529 - val_dice_coef: 0.4036 - val_accuracy: 0.4072 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1721 - jaccard_coef: 0.7078 - dice_coef: 0.8279 - accuracy: 0.8297 - val_loss: 0.5430 - val_jaccard_coef: 0.2962 - val_dice_coef: 0.4570 - val_accuracy: 0.4590 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1613 - jaccard_coef: 0.7233 - dice_coef: 0.8387 - accuracy: 0.8403 - val_loss: 0.3108 - val_jaccard_coef: 0.5259 - val_dice_coef: 0.6892 - val_accuracy: 0.6941 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1543 - jaccard_coef: 0.7334 - dice_coef: 0.8457 - accuracy: 0.8474 - val_loss: 0.4093 - val_jaccard_coef: 0.4197 - val_dice_coef: 0.5907 - val_accuracy: 0.5949 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1462 - jaccard_coef: 0.7455 - dice_coef: 0.8538 - accuracy: 0.8554 - val_loss: 0.2774 - val_jaccard_coef: 0.5657 - val_dice_coef: 0.7226 - val_accuracy: 0.7254 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1420 - jaccard_coef: 0.7520 - dice_coef: 0.8580 - accuracy: 0.8595 - val_loss: 0.2373 - val_jaccard_coef: 0.6166 - val_dice_coef: 0.7627 - val_accuracy: 0.7645 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.1409 - jaccard_coef: 0.7541 - dice_coef: 0.8591 - accuracy: 0.8608 - val_loss: 0.2184 - val_jaccard_coef: 0.6422 - val_dice_coef: 0.7816 - val_accuracy: 0.7833 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1416 - jaccard_coef: 0.7528 - dice_coef: 0.8584 - accuracy: 0.8601 - val_loss: 0.2137 - val_jaccard_coef: 0.6486 - val_dice_coef: 0.7863 - val_accuracy: 0.7886 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1372 - jaccard_coef: 0.7601 - dice_coef: 0.8628 - accuracy: 0.8643 - val_loss: 0.1875 - val_jaccard_coef: 0.6844 - val_dice_coef: 0.8125 - val_accuracy: 0.8142 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1328 - jaccard_coef: 0.7661 - dice_coef: 0.8672 - accuracy: 0.8687 - val_loss: 0.1879 - val_jaccard_coef: 0.6839 - val_dice_coef: 0.8121 - val_accuracy: 0.8138 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1298 - jaccard_coef: 0.7709 - dice_coef: 0.8702 - accuracy: 0.8717 - val_loss: 0.1773 - val_jaccard_coef: 0.6990 - val_dice_coef: 0.8227 - val_accuracy: 0.8245 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1288 - jaccard_coef: 0.7725 - dice_coef: 0.8712 - accuracy: 0.8725 - val_loss: 0.1745 - val_jaccard_coef: 0.7031 - val_dice_coef: 0.8255 - val_accuracy: 0.8273 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1268 - jaccard_coef: 0.7759 - dice_coef: 0.8732 - accuracy: 0.8746 - val_loss: 0.1707 - val_jaccard_coef: 0.7086 - val_dice_coef: 0.8293 - val_accuracy: 0.8312 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1250 - jaccard_coef: 0.7786 - dice_coef: 0.8750 - accuracy: 0.8764 - val_loss: 0.1723 - val_jaccard_coef: 0.7062 - val_dice_coef: 0.8277 - val_accuracy: 0.8292 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1246 - jaccard_coef: 0.7790 - dice_coef: 0.8754 - accuracy: 0.8768 - val_loss: 0.1737 - val_jaccard_coef: 0.7041 - val_dice_coef: 0.8263 - val_accuracy: 0.8279 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1240 - jaccard_coef: 0.7799 - dice_coef: 0.8760 - accuracy: 0.8774 - val_loss: 0.1717 - val_jaccard_coef: 0.7070 - val_dice_coef: 0.8283 - val_accuracy: 0.8297 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1237 - jaccard_coef: 0.7806 - dice_coef: 0.8763 - accuracy: 0.8776 - val_loss: 0.1709 - val_jaccard_coef: 0.7083 - val_dice_coef: 0.8291 - val_accuracy: 0.8309 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1236 - jaccard_coef: 0.7806 - dice_coef: 0.8764 - accuracy: 0.8778 - val_loss: 0.1676 - val_jaccard_coef: 0.7131 - val_dice_coef: 0.8324 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1231 - jaccard_coef: 0.7814 - dice_coef: 0.8769 - accuracy: 0.8783 - val_loss: 0.1728 - val_jaccard_coef: 0.7057 - val_dice_coef: 0.8272 - val_accuracy: 0.8285 - lr: 0.0010\n",
            "Epoch 68/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1227 - jaccard_coef: 0.7823 - dice_coef: 0.8773 - accuracy: 0.8786 - val_loss: 0.1704 - val_jaccard_coef: 0.7090 - val_dice_coef: 0.8296 - val_accuracy: 0.8308 - lr: 0.0010\n",
            "Epoch 69/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1217 - jaccard_coef: 0.7841 - dice_coef: 0.8783 - accuracy: 0.8797 - val_loss: 0.1681 - val_jaccard_coef: 0.7125 - val_dice_coef: 0.8319 - val_accuracy: 0.8333 - lr: 0.0010\n",
            "Epoch 70/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1213 - jaccard_coef: 0.7846 - dice_coef: 0.8787 - accuracy: 0.8801 - val_loss: 0.1720 - val_jaccard_coef: 0.7067 - val_dice_coef: 0.8280 - val_accuracy: 0.8293 - lr: 0.0010\n",
            "Epoch 71/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1208 - jaccard_coef: 0.7850 - dice_coef: 0.8792 - accuracy: 0.8805 - val_loss: 0.1689 - val_jaccard_coef: 0.7113 - val_dice_coef: 0.8311 - val_accuracy: 0.8319 - lr: 0.0010\n",
            "Epoch 72/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1205 - jaccard_coef: 0.7861 - dice_coef: 0.8795 - accuracy: 0.8808 - val_loss: 0.1658 - val_jaccard_coef: 0.7159 - val_dice_coef: 0.8342 - val_accuracy: 0.8354 - lr: 0.0010\n",
            "Epoch 73/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1206 - jaccard_coef: 0.7856 - dice_coef: 0.8794 - accuracy: 0.8806 - val_loss: 0.1669 - val_jaccard_coef: 0.7142 - val_dice_coef: 0.8331 - val_accuracy: 0.8343 - lr: 0.0010\n",
            "Epoch 74/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1201 - jaccard_coef: 0.7862 - dice_coef: 0.8799 - accuracy: 0.8812 - val_loss: 0.1662 - val_jaccard_coef: 0.7151 - val_dice_coef: 0.8338 - val_accuracy: 0.8353 - lr: 0.0010\n",
            "Epoch 75/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1203 - jaccard_coef: 0.7861 - dice_coef: 0.8797 - accuracy: 0.8811 - val_loss: 0.1647 - val_jaccard_coef: 0.7175 - val_dice_coef: 0.8353 - val_accuracy: 0.8366 - lr: 0.0010\n",
            "Epoch 76/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1195 - jaccard_coef: 0.7872 - dice_coef: 0.8805 - accuracy: 0.8818 - val_loss: 0.1664 - val_jaccard_coef: 0.7149 - val_dice_coef: 0.8336 - val_accuracy: 0.8347 - lr: 0.0010\n",
            "Epoch 77/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1194 - jaccard_coef: 0.7880 - dice_coef: 0.8806 - accuracy: 0.8818 - val_loss: 0.1667 - val_jaccard_coef: 0.7144 - val_dice_coef: 0.8333 - val_accuracy: 0.8348 - lr: 0.0010\n",
            "Epoch 78/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1185 - jaccard_coef: 0.7888 - dice_coef: 0.8815 - accuracy: 0.8827 - val_loss: 0.1694 - val_jaccard_coef: 0.7106 - val_dice_coef: 0.8306 - val_accuracy: 0.8317 - lr: 0.0010\n",
            "Epoch 79/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1188 - jaccard_coef: 0.7886 - dice_coef: 0.8812 - accuracy: 0.8825 - val_loss: 0.1685 - val_jaccard_coef: 0.7118 - val_dice_coef: 0.8315 - val_accuracy: 0.8329 - lr: 0.0010\n",
            "Epoch 80/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1184 - jaccard_coef: 0.7889 - dice_coef: 0.8816 - accuracy: 0.8829 - val_loss: 0.1669 - val_jaccard_coef: 0.7142 - val_dice_coef: 0.8331 - val_accuracy: 0.8344 - lr: 0.0010\n",
            "Epoch 81/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1177 - jaccard_coef: 0.7903 - dice_coef: 0.8823 - accuracy: 0.8834 - val_loss: 0.1673 - val_jaccard_coef: 0.7137 - val_dice_coef: 0.8327 - val_accuracy: 0.8340 - lr: 0.0010\n",
            "Epoch 82/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1181 - jaccard_coef: 0.7895 - dice_coef: 0.8819 - accuracy: 0.8830 - val_loss: 0.1665 - val_jaccard_coef: 0.7149 - val_dice_coef: 0.8335 - val_accuracy: 0.8347 - lr: 0.0010\n",
            "Epoch 83/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1179 - jaccard_coef: 0.7901 - dice_coef: 0.8821 - accuracy: 0.8834 - val_loss: 0.1662 - val_jaccard_coef: 0.7153 - val_dice_coef: 0.8338 - val_accuracy: 0.8348 - lr: 0.0010\n",
            "Epoch 84/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1175 - jaccard_coef: 0.7903 - dice_coef: 0.8825 - accuracy: 0.8837 - val_loss: 0.1668 - val_jaccard_coef: 0.7145 - val_dice_coef: 0.8332 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 85/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1175 - jaccard_coef: 0.7905 - dice_coef: 0.8825 - accuracy: 0.8836 - val_loss: 0.1664 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8336 - val_accuracy: 0.8347 - lr: 0.0010\n",
            "Epoch 86/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1174 - jaccard_coef: 0.7907 - dice_coef: 0.8826 - accuracy: 0.8838 - val_loss: 0.1650 - val_jaccard_coef: 0.7170 - val_dice_coef: 0.8350 - val_accuracy: 0.8361 - lr: 0.0010\n",
            "Epoch 87/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1171 - jaccard_coef: 0.7912 - dice_coef: 0.8829 - accuracy: 0.8841 - val_loss: 0.1663 - val_jaccard_coef: 0.7152 - val_dice_coef: 0.8337 - val_accuracy: 0.8346 - lr: 0.0010\n",
            "Epoch 88/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1166 - jaccard_coef: 0.7917 - dice_coef: 0.8834 - accuracy: 0.8845 - val_loss: 0.1662 - val_jaccard_coef: 0.7152 - val_dice_coef: 0.8338 - val_accuracy: 0.8348 - lr: 0.0010\n",
            "Epoch 89/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1165 - jaccard_coef: 0.7922 - dice_coef: 0.8835 - accuracy: 0.8846 - val_loss: 0.1674 - val_jaccard_coef: 0.7136 - val_dice_coef: 0.8326 - val_accuracy: 0.8336 - lr: 0.0010\n",
            "Epoch 90/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1162 - jaccard_coef: 0.7926 - dice_coef: 0.8838 - accuracy: 0.8849 - val_loss: 0.1644 - val_jaccard_coef: 0.7180 - val_dice_coef: 0.8356 - val_accuracy: 0.8365 - lr: 0.0010\n",
            "Epoch 91/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1159 - jaccard_coef: 0.7929 - dice_coef: 0.8841 - accuracy: 0.8852 - val_loss: 0.1663 - val_jaccard_coef: 0.7151 - val_dice_coef: 0.8337 - val_accuracy: 0.8347 - lr: 0.0010\n",
            "Epoch 92/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1161 - jaccard_coef: 0.7927 - dice_coef: 0.8839 - accuracy: 0.8850 - val_loss: 0.1682 - val_jaccard_coef: 0.7123 - val_dice_coef: 0.8318 - val_accuracy: 0.8326 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "18/18 [==============================] - 2s 113ms/step - loss: 0.1155 - jaccard_coef: 0.7938 - dice_coef: 0.8845 - accuracy: 0.8856 - val_loss: 0.1679 - val_jaccard_coef: 0.7127 - val_dice_coef: 0.8321 - val_accuracy: 0.8330 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1156 - jaccard_coef: 0.7939 - dice_coef: 0.8844 - accuracy: 0.8854 - val_loss: 0.1660 - val_jaccard_coef: 0.7156 - val_dice_coef: 0.8340 - val_accuracy: 0.8350 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1154 - jaccard_coef: 0.7938 - dice_coef: 0.8846 - accuracy: 0.8856 - val_loss: 0.1671 - val_jaccard_coef: 0.7140 - val_dice_coef: 0.8329 - val_accuracy: 0.8339 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1141 - jaccard_coef: 0.7958 - dice_coef: 0.8859 - accuracy: 0.8873 - val_loss: 0.1702 - val_jaccard_coef: 0.7095 - val_dice_coef: 0.8298 - val_accuracy: 0.8308 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1143 - jaccard_coef: 0.7956 - dice_coef: 0.8857 - accuracy: 0.8871 - val_loss: 0.1672 - val_jaccard_coef: 0.7137 - val_dice_coef: 0.8328 - val_accuracy: 0.8340 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "18/18 [==============================] - 2s 123ms/step - loss: 0.1145 - jaccard_coef: 0.7953 - dice_coef: 0.8855 - accuracy: 0.8869 - val_loss: 0.1666 - val_jaccard_coef: 0.7146 - val_dice_coef: 0.8334 - val_accuracy: 0.8347 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1143 - jaccard_coef: 0.7958 - dice_coef: 0.8857 - accuracy: 0.8869 - val_loss: 0.1687 - val_jaccard_coef: 0.7117 - val_dice_coef: 0.8313 - val_accuracy: 0.8322 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1150 - jaccard_coef: 0.7943 - dice_coef: 0.8850 - accuracy: 0.8863 - val_loss: 0.1732 - val_jaccard_coef: 0.7051 - val_dice_coef: 0.8268 - val_accuracy: 0.8278 - lr: 0.0010\n",
            "Epoch 101/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1140 - jaccard_coef: 0.7961 - dice_coef: 0.8860 - accuracy: 0.8872 - val_loss: 0.1642 - val_jaccard_coef: 0.7181 - val_dice_coef: 0.8358 - val_accuracy: 0.8366 - lr: 0.0010\n",
            "Epoch 102/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1123 - jaccard_coef: 0.7989 - dice_coef: 0.8877 - accuracy: 0.8888 - val_loss: 0.1649 - val_jaccard_coef: 0.7172 - val_dice_coef: 0.8351 - val_accuracy: 0.8359 - lr: 0.0010\n",
            "Epoch 103/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1107 - jaccard_coef: 0.8013 - dice_coef: 0.8893 - accuracy: 0.8904 - val_loss: 0.1679 - val_jaccard_coef: 0.7129 - val_dice_coef: 0.8321 - val_accuracy: 0.8330 - lr: 0.0010\n",
            "Epoch 104/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1101 - jaccard_coef: 0.8028 - dice_coef: 0.8899 - accuracy: 0.8910 - val_loss: 0.1672 - val_jaccard_coef: 0.7138 - val_dice_coef: 0.8328 - val_accuracy: 0.8337 - lr: 0.0010\n",
            "Epoch 105/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1101 - jaccard_coef: 0.8025 - dice_coef: 0.8899 - accuracy: 0.8910 - val_loss: 0.1662 - val_jaccard_coef: 0.7153 - val_dice_coef: 0.8338 - val_accuracy: 0.8346 - lr: 0.0010\n",
            "Epoch 106/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1095 - jaccard_coef: 0.8032 - dice_coef: 0.8905 - accuracy: 0.8916 - val_loss: 0.1677 - val_jaccard_coef: 0.7131 - val_dice_coef: 0.8323 - val_accuracy: 0.8331 - lr: 0.0010\n",
            "Epoch 107/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1094 - jaccard_coef: 0.8032 - dice_coef: 0.8906 - accuracy: 0.8917 - val_loss: 0.1671 - val_jaccard_coef: 0.7141 - val_dice_coef: 0.8329 - val_accuracy: 0.8339 - lr: 0.0010\n",
            "Epoch 108/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1093 - jaccard_coef: 0.8042 - dice_coef: 0.8907 - accuracy: 0.8918 - val_loss: 0.1661 - val_jaccard_coef: 0.7155 - val_dice_coef: 0.8339 - val_accuracy: 0.8344 - lr: 0.0010\n",
            "Epoch 109/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1089 - jaccard_coef: 0.8047 - dice_coef: 0.8911 - accuracy: 0.8922 - val_loss: 0.1649 - val_jaccard_coef: 0.7172 - val_dice_coef: 0.8351 - val_accuracy: 0.8360 - lr: 0.0010\n",
            "Epoch 110/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1087 - jaccard_coef: 0.8045 - dice_coef: 0.8913 - accuracy: 0.8924 - val_loss: 0.1661 - val_jaccard_coef: 0.7155 - val_dice_coef: 0.8339 - val_accuracy: 0.8347 - lr: 0.0010\n",
            "Epoch 111/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1083 - jaccard_coef: 0.8055 - dice_coef: 0.8917 - accuracy: 0.8928 - val_loss: 0.1655 - val_jaccard_coef: 0.7165 - val_dice_coef: 0.8345 - val_accuracy: 0.8354 - lr: 0.0010\n",
            "Epoch 112/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1076 - jaccard_coef: 0.8064 - dice_coef: 0.8924 - accuracy: 0.8934 - val_loss: 0.1687 - val_jaccard_coef: 0.7117 - val_dice_coef: 0.8313 - val_accuracy: 0.8321 - lr: 0.0010\n",
            "Epoch 113/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1080 - jaccard_coef: 0.8060 - dice_coef: 0.8920 - accuracy: 0.8930 - val_loss: 0.1664 - val_jaccard_coef: 0.7151 - val_dice_coef: 0.8336 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 114/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1084 - jaccard_coef: 0.8053 - dice_coef: 0.8916 - accuracy: 0.8926 - val_loss: 0.1706 - val_jaccard_coef: 0.7091 - val_dice_coef: 0.8294 - val_accuracy: 0.8300 - lr: 0.0010\n",
            "Epoch 115/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1083 - jaccard_coef: 0.8056 - dice_coef: 0.8917 - accuracy: 0.8927 - val_loss: 0.1670 - val_jaccard_coef: 0.7143 - val_dice_coef: 0.8330 - val_accuracy: 0.8338 - lr: 0.0010\n",
            "Epoch 116/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1085 - jaccard_coef: 0.8049 - dice_coef: 0.8915 - accuracy: 0.8925 - val_loss: 0.1645 - val_jaccard_coef: 0.7178 - val_dice_coef: 0.8355 - val_accuracy: 0.8362 - lr: 0.0010\n",
            "Epoch 117/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1080 - jaccard_coef: 0.8058 - dice_coef: 0.8920 - accuracy: 0.8930 - val_loss: 0.1678 - val_jaccard_coef: 0.7131 - val_dice_coef: 0.8322 - val_accuracy: 0.8330 - lr: 0.0010\n",
            "Epoch 118/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1074 - jaccard_coef: 0.8066 - dice_coef: 0.8926 - accuracy: 0.8935 - val_loss: 0.1652 - val_jaccard_coef: 0.7169 - val_dice_coef: 0.8348 - val_accuracy: 0.8358 - lr: 0.0010\n",
            "Epoch 119/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1077 - jaccard_coef: 0.8063 - dice_coef: 0.8923 - accuracy: 0.8933 - val_loss: 0.1688 - val_jaccard_coef: 0.7117 - val_dice_coef: 0.8312 - val_accuracy: 0.8323 - lr: 0.0010\n",
            "Epoch 120/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1079 - jaccard_coef: 0.8067 - dice_coef: 0.8921 - accuracy: 0.8931 - val_loss: 0.1666 - val_jaccard_coef: 0.7148 - val_dice_coef: 0.8334 - val_accuracy: 0.8341 - lr: 0.0010\n",
            "Epoch 121/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1075 - jaccard_coef: 0.8062 - dice_coef: 0.8925 - accuracy: 0.8934\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1075 - jaccard_coef: 0.8062 - dice_coef: 0.8925 - accuracy: 0.8934 - val_loss: 0.1669 - val_jaccard_coef: 0.7144 - val_dice_coef: 0.8331 - val_accuracy: 0.8338 - lr: 0.0010\n",
            "Epoch 122/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1063 - jaccard_coef: 0.8089 - dice_coef: 0.8937 - accuracy: 0.8947 - val_loss: 0.1653 - val_jaccard_coef: 0.7167 - val_dice_coef: 0.8347 - val_accuracy: 0.8355 - lr: 3.0000e-04\n",
            "Epoch 123/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1051 - jaccard_coef: 0.8105 - dice_coef: 0.8949 - accuracy: 0.8959 - val_loss: 0.1644 - val_jaccard_coef: 0.7181 - val_dice_coef: 0.8356 - val_accuracy: 0.8364 - lr: 3.0000e-04\n",
            "Epoch 124/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1047 - jaccard_coef: 0.8116 - dice_coef: 0.8953 - accuracy: 0.8963 - val_loss: 0.1643 - val_jaccard_coef: 0.7182 - val_dice_coef: 0.8357 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
            "Epoch 125/200\n",
            "18/18 [==============================] - 2s 114ms/step - loss: 0.1043 - jaccard_coef: 0.8116 - dice_coef: 0.8957 - accuracy: 0.8967 - val_loss: 0.1644 - val_jaccard_coef: 0.7180 - val_dice_coef: 0.8356 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
            "Epoch 126/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1043 - jaccard_coef: 0.8117 - dice_coef: 0.8957 - accuracy: 0.8968 - val_loss: 0.1646 - val_jaccard_coef: 0.7178 - val_dice_coef: 0.8354 - val_accuracy: 0.8364 - lr: 3.0000e-04\n",
            "Epoch 127/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1041 - jaccard_coef: 0.8120 - dice_coef: 0.8959 - accuracy: 0.8970 - val_loss: 0.1637 - val_jaccard_coef: 0.7190 - val_dice_coef: 0.8363 - val_accuracy: 0.8370 - lr: 3.0000e-04\n",
            "Epoch 128/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1040 - jaccard_coef: 0.8124 - dice_coef: 0.8960 - accuracy: 0.8971 - val_loss: 0.1638 - val_jaccard_coef: 0.7189 - val_dice_coef: 0.8362 - val_accuracy: 0.8371 - lr: 3.0000e-04\n",
            "Epoch 129/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1040 - jaccard_coef: 0.8125 - dice_coef: 0.8960 - accuracy: 0.8971 - val_loss: 0.1641 - val_jaccard_coef: 0.7185 - val_dice_coef: 0.8359 - val_accuracy: 0.8368 - lr: 3.0000e-04\n",
            "Epoch 130/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1039 - jaccard_coef: 0.8128 - dice_coef: 0.8961 - accuracy: 0.8972 - val_loss: 0.1642 - val_jaccard_coef: 0.7183 - val_dice_coef: 0.8358 - val_accuracy: 0.8368 - lr: 3.0000e-04\n",
            "Epoch 131/200\n",
            "18/18 [==============================] - 2s 115ms/step - loss: 0.1038 - jaccard_coef: 0.8130 - dice_coef: 0.8962 - accuracy: 0.8973 - val_loss: 0.1644 - val_jaccard_coef: 0.7180 - val_dice_coef: 0.8356 - val_accuracy: 0.8364 - lr: 3.0000e-04\n",
            "Epoch 132/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1038 - jaccard_coef: 0.8131 - dice_coef: 0.8962 - accuracy: 0.8973 - val_loss: 0.1643 - val_jaccard_coef: 0.7181 - val_dice_coef: 0.8357 - val_accuracy: 0.8366 - lr: 3.0000e-04\n",
            "Epoch 133/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1036 - jaccard_coef: 0.8132 - dice_coef: 0.8964 - accuracy: 0.8976 - val_loss: 0.1647 - val_jaccard_coef: 0.7175 - val_dice_coef: 0.8353 - val_accuracy: 0.8360 - lr: 3.0000e-04\n",
            "Epoch 134/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1037 - jaccard_coef: 0.8132 - dice_coef: 0.8963 - accuracy: 0.8974 - val_loss: 0.1647 - val_jaccard_coef: 0.7176 - val_dice_coef: 0.8353 - val_accuracy: 0.8361 - lr: 3.0000e-04\n",
            "Epoch 135/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1036 - jaccard_coef: 0.8131 - dice_coef: 0.8964 - accuracy: 0.8975 - val_loss: 0.1634 - val_jaccard_coef: 0.7194 - val_dice_coef: 0.8366 - val_accuracy: 0.8375 - lr: 3.0000e-04\n",
            "Epoch 136/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.1034 - jaccard_coef: 0.8134 - dice_coef: 0.8966 - accuracy: 0.8977 - val_loss: 0.1634 - val_jaccard_coef: 0.7195 - val_dice_coef: 0.8366 - val_accuracy: 0.8375 - lr: 3.0000e-04\n",
            "Epoch 137/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1032 - jaccard_coef: 0.8142 - dice_coef: 0.8968 - accuracy: 0.8979 - val_loss: 0.1641 - val_jaccard_coef: 0.7184 - val_dice_coef: 0.8359 - val_accuracy: 0.8368 - lr: 3.0000e-04\n",
            "Epoch 138/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1034 - jaccard_coef: 0.8134 - dice_coef: 0.8966 - accuracy: 0.8977 - val_loss: 0.1634 - val_jaccard_coef: 0.7194 - val_dice_coef: 0.8366 - val_accuracy: 0.8375 - lr: 3.0000e-04\n",
            "Epoch 139/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1033 - jaccard_coef: 0.8142 - dice_coef: 0.8967 - accuracy: 0.8978 - val_loss: 0.1641 - val_jaccard_coef: 0.7184 - val_dice_coef: 0.8359 - val_accuracy: 0.8367 - lr: 3.0000e-04\n",
            "Epoch 140/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1034 - jaccard_coef: 0.8130 - dice_coef: 0.8966 - accuracy: 0.8976 - val_loss: 0.1645 - val_jaccard_coef: 0.7179 - val_dice_coef: 0.8355 - val_accuracy: 0.8364 - lr: 3.0000e-04\n",
            "Epoch 141/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.1033 - jaccard_coef: 0.8135 - dice_coef: 0.8967 - accuracy: 0.8978 - val_loss: 0.1645 - val_jaccard_coef: 0.7179 - val_dice_coef: 0.8355 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
            "Epoch 142/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1032 - jaccard_coef: 0.8135 - dice_coef: 0.8968 - accuracy: 0.8979 - val_loss: 0.1646 - val_jaccard_coef: 0.7177 - val_dice_coef: 0.8354 - val_accuracy: 0.8364 - lr: 3.0000e-04\n",
            "Epoch 143/200\n",
            "18/18 [==============================] - 2s 121ms/step - loss: 0.1031 - jaccard_coef: 0.8137 - dice_coef: 0.8969 - accuracy: 0.8980 - val_loss: 0.1651 - val_jaccard_coef: 0.7171 - val_dice_coef: 0.8349 - val_accuracy: 0.8358 - lr: 3.0000e-04\n",
            "Epoch 144/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1032 - jaccard_coef: 0.8139 - dice_coef: 0.8968 - accuracy: 0.8979 - val_loss: 0.1655 - val_jaccard_coef: 0.7165 - val_dice_coef: 0.8345 - val_accuracy: 0.8353 - lr: 3.0000e-04\n",
            "Epoch 145/200\n",
            "18/18 [==============================] - 2s 120ms/step - loss: 0.1031 - jaccard_coef: 0.8140 - dice_coef: 0.8969 - accuracy: 0.8980 - val_loss: 0.1649 - val_jaccard_coef: 0.7172 - val_dice_coef: 0.8351 - val_accuracy: 0.8360 - lr: 3.0000e-04\n",
            "Epoch 146/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.1030 - jaccard_coef: 0.8146 - dice_coef: 0.8970 - accuracy: 0.8980 - val_loss: 0.1646 - val_jaccard_coef: 0.7178 - val_dice_coef: 0.8354 - val_accuracy: 0.8363 - lr: 3.0000e-04\n",
            "Epoch 147/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1028 - jaccard_coef: 0.8146 - dice_coef: 0.8972 - accuracy: 0.8983 - val_loss: 0.1648 - val_jaccard_coef: 0.7174 - val_dice_coef: 0.8352 - val_accuracy: 0.8361 - lr: 3.0000e-04\n",
            "Epoch 148/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.1029 - jaccard_coef: 0.8143 - dice_coef: 0.8971 - accuracy: 0.8981 - val_loss: 0.1648 - val_jaccard_coef: 0.7174 - val_dice_coef: 0.8352 - val_accuracy: 0.8360 - lr: 3.0000e-04\n",
            "Epoch 149/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1028 - jaccard_coef: 0.8142 - dice_coef: 0.8972 - accuracy: 0.8983 - val_loss: 0.1657 - val_jaccard_coef: 0.7161 - val_dice_coef: 0.8343 - val_accuracy: 0.8351 - lr: 3.0000e-04\n",
            "Epoch 150/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1028 - jaccard_coef: 0.8144 - dice_coef: 0.8972 - accuracy: 0.8983 - val_loss: 0.1653 - val_jaccard_coef: 0.7167 - val_dice_coef: 0.8347 - val_accuracy: 0.8354 - lr: 3.0000e-04\n",
            "Epoch 151/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1027 - jaccard_coef: 0.8153 - dice_coef: 0.8973 - accuracy: 0.8983 - val_loss: 0.1651 - val_jaccard_coef: 0.7170 - val_dice_coef: 0.8349 - val_accuracy: 0.8357 - lr: 3.0000e-04\n",
            "Epoch 152/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1029 - jaccard_coef: 0.8142 - dice_coef: 0.8971 - accuracy: 0.8981 - val_loss: 0.1650 - val_jaccard_coef: 0.7171 - val_dice_coef: 0.8350 - val_accuracy: 0.8357 - lr: 3.0000e-04\n",
            "Epoch 153/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1027 - jaccard_coef: 0.8142 - dice_coef: 0.8973 - accuracy: 0.8983 - val_loss: 0.1646 - val_jaccard_coef: 0.7176 - val_dice_coef: 0.8354 - val_accuracy: 0.8362 - lr: 3.0000e-04\n",
            "Epoch 154/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1025 - jaccard_coef: 0.8147 - dice_coef: 0.8975 - accuracy: 0.8985 - val_loss: 0.1647 - val_jaccard_coef: 0.7175 - val_dice_coef: 0.8353 - val_accuracy: 0.8360 - lr: 3.0000e-04\n",
            "Epoch 155/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.1025 - jaccard_coef: 0.8145 - dice_coef: 0.8975 - accuracy: 0.8986\n",
            "Epoch 00155: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1025 - jaccard_coef: 0.8145 - dice_coef: 0.8975 - accuracy: 0.8986 - val_loss: 0.1651 - val_jaccard_coef: 0.7170 - val_dice_coef: 0.8349 - val_accuracy: 0.8358 - lr: 3.0000e-04\n",
            "Epoch 156/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.1015 - jaccard_coef: 0.8165 - dice_coef: 0.8985 - accuracy: 0.8997 - val_loss: 0.1650 - val_jaccard_coef: 0.7170 - val_dice_coef: 0.8350 - val_accuracy: 0.8358 - lr: 9.0000e-05\n",
            "Epoch 157/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.1008 - jaccard_coef: 0.8175 - dice_coef: 0.8992 - accuracy: 0.9006 - val_loss: 0.1647 - val_jaccard_coef: 0.7176 - val_dice_coef: 0.8353 - val_accuracy: 0.8361 - lr: 9.0000e-05\n",
            "Epoch 158/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.1000 - jaccard_coef: 0.8188 - dice_coef: 0.9000 - accuracy: 0.9013 - val_loss: 0.1648 - val_jaccard_coef: 0.7175 - val_dice_coef: 0.8352 - val_accuracy: 0.8361 - lr: 9.0000e-05\n",
            "Epoch 159/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0997 - jaccard_coef: 0.8198 - dice_coef: 0.9003 - accuracy: 0.9016 - val_loss: 0.1651 - val_jaccard_coef: 0.7170 - val_dice_coef: 0.8349 - val_accuracy: 0.8358 - lr: 9.0000e-05\n",
            "Epoch 160/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0992 - jaccard_coef: 0.8202 - dice_coef: 0.9008 - accuracy: 0.9020 - val_loss: 0.1648 - val_jaccard_coef: 0.7174 - val_dice_coef: 0.8352 - val_accuracy: 0.8360 - lr: 9.0000e-05\n",
            "Epoch 161/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0992 - jaccard_coef: 0.8200 - dice_coef: 0.9008 - accuracy: 0.9021 - val_loss: 0.1647 - val_jaccard_coef: 0.7176 - val_dice_coef: 0.8353 - val_accuracy: 0.8360 - lr: 9.0000e-05\n",
            "Epoch 162/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0991 - jaccard_coef: 0.8206 - dice_coef: 0.9009 - accuracy: 0.9021 - val_loss: 0.1648 - val_jaccard_coef: 0.7174 - val_dice_coef: 0.8352 - val_accuracy: 0.8360 - lr: 9.0000e-05\n",
            "Epoch 163/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0988 - jaccard_coef: 0.8210 - dice_coef: 0.9012 - accuracy: 0.9024 - val_loss: 0.1650 - val_jaccard_coef: 0.7171 - val_dice_coef: 0.8350 - val_accuracy: 0.8359 - lr: 9.0000e-05\n",
            "Epoch 164/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0986 - jaccard_coef: 0.8210 - dice_coef: 0.9014 - accuracy: 0.9026 - val_loss: 0.1648 - val_jaccard_coef: 0.7174 - val_dice_coef: 0.8352 - val_accuracy: 0.8362 - lr: 9.0000e-05\n",
            "Epoch 165/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0988 - jaccard_coef: 0.8211 - dice_coef: 0.9012 - accuracy: 0.9025 - val_loss: 0.1650 - val_jaccard_coef: 0.7173 - val_dice_coef: 0.8350 - val_accuracy: 0.8361 - lr: 9.0000e-05\n",
            "Epoch 166/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0984 - jaccard_coef: 0.8214 - dice_coef: 0.9016 - accuracy: 0.9028 - val_loss: 0.1646 - val_jaccard_coef: 0.7179 - val_dice_coef: 0.8354 - val_accuracy: 0.8365 - lr: 9.0000e-05\n",
            "Epoch 167/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0984 - jaccard_coef: 0.8218 - dice_coef: 0.9016 - accuracy: 0.9029 - val_loss: 0.1642 - val_jaccard_coef: 0.7185 - val_dice_coef: 0.8358 - val_accuracy: 0.8372 - lr: 9.0000e-05\n",
            "Epoch 168/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0983 - jaccard_coef: 0.8217 - dice_coef: 0.9017 - accuracy: 0.9030 - val_loss: 0.1646 - val_jaccard_coef: 0.7179 - val_dice_coef: 0.8354 - val_accuracy: 0.8366 - lr: 9.0000e-05\n",
            "Epoch 169/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0981 - jaccard_coef: 0.8221 - dice_coef: 0.9019 - accuracy: 0.9031 - val_loss: 0.1647 - val_jaccard_coef: 0.7177 - val_dice_coef: 0.8353 - val_accuracy: 0.8367 - lr: 9.0000e-05\n",
            "Epoch 170/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0981 - jaccard_coef: 0.8221 - dice_coef: 0.9019 - accuracy: 0.9032 - val_loss: 0.1648 - val_jaccard_coef: 0.7175 - val_dice_coef: 0.8352 - val_accuracy: 0.8365 - lr: 9.0000e-05\n",
            "Epoch 171/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0978 - jaccard_coef: 0.8225 - dice_coef: 0.9022 - accuracy: 0.9034 - val_loss: 0.1654 - val_jaccard_coef: 0.7167 - val_dice_coef: 0.8346 - val_accuracy: 0.8361 - lr: 9.0000e-05\n",
            "Epoch 172/200\n",
            "18/18 [==============================] - 2s 128ms/step - loss: 0.0978 - jaccard_coef: 0.8223 - dice_coef: 0.9022 - accuracy: 0.9035 - val_loss: 0.1653 - val_jaccard_coef: 0.7168 - val_dice_coef: 0.8347 - val_accuracy: 0.8361 - lr: 9.0000e-05\n",
            "Epoch 173/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0976 - jaccard_coef: 0.8229 - dice_coef: 0.9024 - accuracy: 0.9036 - val_loss: 0.1655 - val_jaccard_coef: 0.7166 - val_dice_coef: 0.8345 - val_accuracy: 0.8359 - lr: 9.0000e-05\n",
            "Epoch 174/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0975 - jaccard_coef: 0.8231 - dice_coef: 0.9025 - accuracy: 0.9038 - val_loss: 0.1660 - val_jaccard_coef: 0.7159 - val_dice_coef: 0.8340 - val_accuracy: 0.8354 - lr: 9.0000e-05\n",
            "Epoch 175/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0974 - jaccard_coef: 0.8228 - dice_coef: 0.9026 - accuracy: 0.9038\n",
            "Epoch 00175: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0974 - jaccard_coef: 0.8228 - dice_coef: 0.9026 - accuracy: 0.9038 - val_loss: 0.1664 - val_jaccard_coef: 0.7153 - val_dice_coef: 0.8336 - val_accuracy: 0.8350 - lr: 9.0000e-05\n",
            "Epoch 176/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0974 - jaccard_coef: 0.8230 - dice_coef: 0.9026 - accuracy: 0.9039 - val_loss: 0.1665 - val_jaccard_coef: 0.7151 - val_dice_coef: 0.8335 - val_accuracy: 0.8350 - lr: 2.7000e-05\n",
            "Epoch 177/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0973 - jaccard_coef: 0.8237 - dice_coef: 0.9027 - accuracy: 0.9040 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8346 - lr: 2.7000e-05\n",
            "Epoch 178/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0973 - jaccard_coef: 0.8235 - dice_coef: 0.9027 - accuracy: 0.9040 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8346 - lr: 2.7000e-05\n",
            "Epoch 179/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0971 - jaccard_coef: 0.8242 - dice_coef: 0.9029 - accuracy: 0.9042 - val_loss: 0.1664 - val_jaccard_coef: 0.7153 - val_dice_coef: 0.8336 - val_accuracy: 0.8348 - lr: 2.7000e-05\n",
            "Epoch 180/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0972 - jaccard_coef: 0.8233 - dice_coef: 0.9028 - accuracy: 0.9041 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8347 - lr: 2.7000e-05\n",
            "Epoch 181/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0972 - jaccard_coef: 0.8238 - dice_coef: 0.9028 - accuracy: 0.9040 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8347 - lr: 2.7000e-05\n",
            "Epoch 182/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0971 - jaccard_coef: 0.8236 - dice_coef: 0.9029 - accuracy: 0.9041 - val_loss: 0.1667 - val_jaccard_coef: 0.7149 - val_dice_coef: 0.8333 - val_accuracy: 0.8345 - lr: 2.7000e-05\n",
            "Epoch 183/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0972 - jaccard_coef: 0.8239 - dice_coef: 0.9028 - accuracy: 0.9040 - val_loss: 0.1667 - val_jaccard_coef: 0.7148 - val_dice_coef: 0.8333 - val_accuracy: 0.8345 - lr: 2.7000e-05\n",
            "Epoch 184/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0971 - jaccard_coef: 0.8237 - dice_coef: 0.9029 - accuracy: 0.9041 - val_loss: 0.1668 - val_jaccard_coef: 0.7147 - val_dice_coef: 0.8332 - val_accuracy: 0.8345 - lr: 2.7000e-05\n",
            "Epoch 185/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0970 - jaccard_coef: 0.8236 - dice_coef: 0.9030 - accuracy: 0.9043 - val_loss: 0.1669 - val_jaccard_coef: 0.7146 - val_dice_coef: 0.8331 - val_accuracy: 0.8343 - lr: 2.7000e-05\n",
            "Epoch 186/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0970 - jaccard_coef: 0.8236 - dice_coef: 0.9030 - accuracy: 0.9043 - val_loss: 0.1668 - val_jaccard_coef: 0.7147 - val_dice_coef: 0.8332 - val_accuracy: 0.8344 - lr: 2.7000e-05\n",
            "Epoch 187/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0970 - jaccard_coef: 0.8238 - dice_coef: 0.9030 - accuracy: 0.9042 - val_loss: 0.1665 - val_jaccard_coef: 0.7152 - val_dice_coef: 0.8335 - val_accuracy: 0.8347 - lr: 2.7000e-05\n",
            "Epoch 188/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0969 - jaccard_coef: 0.8243 - dice_coef: 0.9031 - accuracy: 0.9044 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8346 - lr: 2.7000e-05\n",
            "Epoch 189/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0970 - jaccard_coef: 0.8240 - dice_coef: 0.9030 - accuracy: 0.9043 - val_loss: 0.1667 - val_jaccard_coef: 0.7148 - val_dice_coef: 0.8333 - val_accuracy: 0.8345 - lr: 2.7000e-05\n",
            "Epoch 190/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0969 - jaccard_coef: 0.8240 - dice_coef: 0.9031 - accuracy: 0.9043 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8347 - lr: 2.7000e-05\n",
            "Epoch 191/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0970 - jaccard_coef: 0.8239 - dice_coef: 0.9030 - accuracy: 0.9043 - val_loss: 0.1666 - val_jaccard_coef: 0.7150 - val_dice_coef: 0.8334 - val_accuracy: 0.8347 - lr: 2.7000e-05\n",
            "Epoch 192/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0968 - jaccard_coef: 0.8239 - dice_coef: 0.9032 - accuracy: 0.9044 - val_loss: 0.1665 - val_jaccard_coef: 0.7151 - val_dice_coef: 0.8335 - val_accuracy: 0.8347 - lr: 2.7000e-05\n",
            "Epoch 193/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0967 - jaccard_coef: 0.8246 - dice_coef: 0.9033 - accuracy: 0.9046 - val_loss: 0.1664 - val_jaccard_coef: 0.7152 - val_dice_coef: 0.8336 - val_accuracy: 0.8348 - lr: 2.7000e-05\n",
            "Epoch 194/200\n",
            "18/18 [==============================] - 2s 118ms/step - loss: 0.0969 - jaccard_coef: 0.8241 - dice_coef: 0.9031 - accuracy: 0.9043 - val_loss: 0.1664 - val_jaccard_coef: 0.7153 - val_dice_coef: 0.8336 - val_accuracy: 0.8348 - lr: 2.7000e-05\n",
            "Epoch 195/200\n",
            "18/18 [==============================] - ETA: 0s - loss: 0.0967 - jaccard_coef: 0.8244 - dice_coef: 0.9033 - accuracy: 0.9045\n",
            "Epoch 00195: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0967 - jaccard_coef: 0.8244 - dice_coef: 0.9033 - accuracy: 0.9045 - val_loss: 0.1663 - val_jaccard_coef: 0.7154 - val_dice_coef: 0.8337 - val_accuracy: 0.8349 - lr: 2.7000e-05\n",
            "Epoch 196/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0967 - jaccard_coef: 0.8242 - dice_coef: 0.9033 - accuracy: 0.9046 - val_loss: 0.1663 - val_jaccard_coef: 0.7154 - val_dice_coef: 0.8337 - val_accuracy: 0.8349 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0967 - jaccard_coef: 0.8242 - dice_coef: 0.9033 - accuracy: 0.9046 - val_loss: 0.1663 - val_jaccard_coef: 0.7154 - val_dice_coef: 0.8337 - val_accuracy: 0.8349 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "18/18 [==============================] - 2s 116ms/step - loss: 0.0966 - jaccard_coef: 0.8246 - dice_coef: 0.9034 - accuracy: 0.9047 - val_loss: 0.1663 - val_jaccard_coef: 0.7154 - val_dice_coef: 0.8337 - val_accuracy: 0.8349 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "18/18 [==============================] - 2s 119ms/step - loss: 0.0966 - jaccard_coef: 0.8242 - dice_coef: 0.9034 - accuracy: 0.9046 - val_loss: 0.1664 - val_jaccard_coef: 0.7153 - val_dice_coef: 0.8336 - val_accuracy: 0.8348 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "18/18 [==============================] - 2s 117ms/step - loss: 0.0967 - jaccard_coef: 0.8246 - dice_coef: 0.9033 - accuracy: 0.9045 - val_loss: 0.1663 - val_jaccard_coef: 0.7154 - val_dice_coef: 0.8337 - val_accuracy: 0.8348 - lr: 1.0000e-05\n",
            "time:  446.7858064174652\n"
          ]
        }
      ],
      "source": [
        "now = time.time()\n",
        "history1 = model_deeplab.fit(X1, y_train_cat, \n",
        "                    batch_size = 8, \n",
        "                    verbose=1, \n",
        "                    epochs=200, \n",
        "                    validation_data=(X_test, y_test_cat),shuffle=True,\n",
        "                   callbacks = [lrd , mcp])\n",
        "end = time.time()\n",
        "print('time: ', end - now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "20a8d81d",
      "metadata": {
        "id": "20a8d81d"
      },
      "outputs": [],
      "source": [
        "#model_deeplab.save('deeplab.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ce14332f",
      "metadata": {
        "id": "ce14332f"
      },
      "outputs": [],
      "source": [
        "#import pickle\n",
        "#with open('deeplab.pickle' , 'wb') as file:\n",
        "  #pickle.dump(history1.history,file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b16c9fd6",
      "metadata": {
        "id": "b16c9fd6"
      },
      "outputs": [],
      "source": [
        "pred_train = model_deeplab.predict(X1)\n",
        "pred_val = model_deeplab.predict(X_test)\n",
        "pred_test = model_deeplab.predict(test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "5f3b61e4",
      "metadata": {
        "id": "5f3b61e4"
      },
      "outputs": [],
      "source": [
        "pred_train1 = np.argmax(pred_train, axis = 3)\n",
        "pred_val1 = np.argmax(pred_val, axis = 3)\n",
        "pred_test1 = np.argmax(pred_test, axis = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0211e69c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0211e69c",
        "outputId": "1ff3ea3d-86aa-49bb-f3db-42761eadfc16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7549277"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(y_train_cat, pred_train)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a960ae47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a960ae47",
        "outputId": "95d54ae8-9361-4521-ef6e-8af7af909566"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.70722127"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(y_test_cat, pred_val)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "de2548b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de2548b0",
        "outputId": "a98c7af0-4cc2-41cb-fa85-17dc0c518c61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5335441"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "miou = tf.keras.metrics.MeanIoU(num_classes= 31)\n",
        "miou.update_state(test_masks_cat, pred_test)\n",
        "miou.result().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a747672",
      "metadata": {
        "id": "6a747672"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-GPU",
      "language": "python",
      "name": "tf-gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}